{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd04884a46182a36c52786c58ae747f94ef8bde10a8978cfcd06107010b24cc677d",
   "display_name": "Python 3.8.8 64-bit ('ogb-lsc-env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from timeit import default_timer\n",
    "from typing import Tuple\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ogb.lsc import DglPCQM4MDataset\n",
    "from ogb.utils import smiles2graph\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "source": [
    "# Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DglPCQM4MDataset(root='/home/ksadowski/datasets', smiles2graph=smiles2graph)\n",
    "\n",
    "split_dict = dataset.get_idx_split()\n",
    "\n",
    "train_idx = split_dict['train']\n",
    "valid_idx = split_dict['valid']\n",
    "test_idx = split_dict['test']"
   ]
  },
  {
   "source": [
    "# Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feats: int,\n",
    "        out_feats: int,\n",
    "        activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._linear = nn.Linear(in_feats, out_feats)\n",
    "        self._activation = activation\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        x = self._linear(inputs)\n",
    "\n",
    "        if self._activation == 'relu':\n",
    "            x = F.relu(x)\n",
    "        elif self._activation == 'relu6':\n",
    "            x = F.relu6(x)\n",
    "        elif self._activation == 'leaky_relu':\n",
    "            x = F.leaky_relu(x)\n",
    "        elif self._activation == 'elu':\n",
    "            x = F.elu(x)\n",
    "        elif self._activation == 'selu':\n",
    "            x = F.selu(x)\n",
    "        elif self._activation == 'celu':\n",
    "            x = F.celu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BilinearReadoutLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        out_feats: int,\n",
    "        activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._bilinear = nn.Bilinear(node_in_feats, edge_in_feats, out_feats)\n",
    "        self._activation = activation\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> float:\n",
    "        x = self._bilinear(node_inputs, edge_inputs)\n",
    "\n",
    "        if self._activation == 'relu':\n",
    "            x = F.relu(x)\n",
    "        elif self._activation == 'softplus':\n",
    "            x = F.softplus(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MutualMultiAttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        num_heads: int,\n",
    "        short_residual: bool,\n",
    "        dropout_probability: float,\n",
    "        message_aggregation_type: str,\n",
    "        head_pooling_type: str,\n",
    "        linear_projection_activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._node_in_feats = node_in_feats\n",
    "        self._edge_in_feats = edge_in_feats\n",
    "        self._num_heads = num_heads\n",
    "        self._short_residual = short_residual\n",
    "        self._message_aggregation_type = message_aggregation_type\n",
    "        self._head_pooling_type = head_pooling_type\n",
    "        self._node_query_linear = LinearLayer(\n",
    "            node_in_feats,\n",
    "            num_heads * node_in_feats,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "        self._node_key_linear = LinearLayer(\n",
    "            node_in_feats, num_heads, linear_projection_activation)\n",
    "        self._node_value_linear = LinearLayer(\n",
    "            node_in_feats,\n",
    "            num_heads * node_in_feats,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "        self._edge_query_linear = LinearLayer(\n",
    "            edge_in_feats,\n",
    "            num_heads * edge_in_feats,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "        self._edge_key_linear = LinearLayer(\n",
    "            edge_in_feats, num_heads, linear_projection_activation)\n",
    "        self._edge_value_linear = LinearLayer(\n",
    "            edge_in_feats,\n",
    "            num_heads * edge_in_feats,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "        self._node_dropout = nn.Dropout(dropout_probability)\n",
    "        self._edge_dropout = nn.Dropout(dropout_probability)\n",
    "\n",
    "    def _calculate_self_attention(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        in_feats: int,\n",
    "        short_residual: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        if short_residual is not None:\n",
    "            x = query @ torch.transpose(short_residual, -1, -2) @ key\n",
    "        else:\n",
    "            x = query @ torch.transpose(query, -1, -2) @ key\n",
    "\n",
    "        x /= math.sqrt(in_feats)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _create_node_attention_projection(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        edge_self_attention: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        node_attention_projection = torch.zeros(\n",
    "            [self._num_heads, g.num_nodes(), g.num_nodes()])\n",
    "\n",
    "        for edge in range(g.num_edges()):\n",
    "            nodes = g.find_edges(edge)\n",
    "\n",
    "            source = nodes[0].item()\n",
    "            destination = nodes[1].item()\n",
    "\n",
    "            for head in range(self._num_heads):\n",
    "                attention_score = edge_self_attention[head][edge]\n",
    "\n",
    "                node_attention_projection[head][source][destination] = attention_score\n",
    "\n",
    "            return node_attention_projection\n",
    "\n",
    "    def _create_edge_attention_projection(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        lg: dgl.DGLGraph,\n",
    "        node_self_attention: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        edge_attention_projection = torch.zeros(\n",
    "            [self._num_heads, g.num_edges(), g.num_edges()])\n",
    "\n",
    "        for node in range(lg.num_edges()):\n",
    "            edges = lg.find_edges(node)\n",
    "\n",
    "            source = edges[0].item()\n",
    "            destination = edges[1].item()\n",
    "\n",
    "            connecting_node = g.find_edges(source)[1].item()\n",
    "\n",
    "            for head in range(self._num_heads):\n",
    "                attention_score = node_self_attention[head][connecting_node]\n",
    "\n",
    "                edge_attention_projection[head][source][destination] = attention_score\n",
    "\n",
    "            return edge_attention_projection\n",
    "\n",
    "    def _calculate_message_passing(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        value: torch.Tensor,\n",
    "        attention_projection: torch.Tensor,\n",
    "    ):\n",
    "        adjacency = g.adj().to_dense()\n",
    "\n",
    "        if self._message_aggregation_type == 'sum':\n",
    "            x = attention_projection * adjacency\n",
    "        elif self._message_aggregation_type == 'mean':\n",
    "            degree_inv = torch.linalg.inv(torch.diag(g.in_degrees().float()))\n",
    "\n",
    "            x = degree_inv @ attention_projection * adjacency\n",
    "        elif self._message_aggregation_type == 'gcn':\n",
    "            degree_inv_sqrt = torch.sqrt(torch.linalg.inv(\n",
    "                torch.diag(g.in_degrees().float())))\n",
    "            adjacency_inv_sqrt = torch.sqrt(torch.linalg.inv(adjacency))\n",
    "\n",
    "            x = degree_inv_sqrt @ attention_projection * \\\n",
    "                adjacency_inv_sqrt @ degree_inv_sqrt\n",
    "\n",
    "        message_passing = x @ value\n",
    "\n",
    "        return message_passing\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        lg: dgl.DGLGraph,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        node_query = self._node_query_linear(node_inputs)\n",
    "        node_query = node_query.view(self._num_heads, -1, self._node_in_feats)\n",
    "        node_key = self._node_key_linear(node_inputs)\n",
    "        node_key = node_key.view(self._num_heads, -1, 1)\n",
    "        node_value = self._node_value_linear(node_inputs)\n",
    "        node_value = node_value.view(self._num_heads, -1, self._node_in_feats)\n",
    "\n",
    "        edge_query = self._edge_query_linear(edge_inputs)\n",
    "        edge_query = edge_query.view(self._num_heads, -1, self._edge_in_feats)\n",
    "        edge_key = self._edge_key_linear(edge_inputs)\n",
    "        edge_key = edge_key.view(self._num_heads, -1, 1)\n",
    "        edge_value = self._edge_value_linear(edge_inputs)\n",
    "        edge_value = edge_value.view(self._num_heads, -1, self._edge_in_feats)\n",
    "\n",
    "        if self._short_residual:\n",
    "            node_self_attention = self._calculate_self_attention(\n",
    "                node_query, node_key, self._node_in_feats, node_inputs)\n",
    "            edge_self_attention = self._calculate_self_attention(\n",
    "                edge_query, edge_key, self._edge_in_feats, edge_inputs)\n",
    "        else:\n",
    "            node_self_attention = self._calculate_self_attention(\n",
    "                node_query, node_key, self._node_in_feats)\n",
    "            edge_self_attention = self._calculate_self_attention(\n",
    "                edge_query, edge_key, self._edge_in_feats)\n",
    "\n",
    "        node_attention_projection = self._create_node_attention_projection(\n",
    "            g, edge_self_attention)\n",
    "        edge_attention_projection = self._create_edge_attention_projection(\n",
    "            g, lg, node_self_attention)\n",
    "\n",
    "        node_message_passing = self._calculate_message_passing(\n",
    "            g, node_value, node_attention_projection)\n",
    "        edge_message_passing = self._calculate_message_passing(\n",
    "            lg, edge_value, edge_attention_projection)\n",
    "\n",
    "        node_message_passing = self._node_dropout(node_message_passing)\n",
    "        edge_message_passing = self._edge_dropout(edge_message_passing)\n",
    "\n",
    "        if self._head_pooling_type == 'sum':\n",
    "            node_message_passing = node_message_passing.sum(dim=-3)\n",
    "            edge_message_passing = edge_message_passing.sum(dim=-3)\n",
    "        elif self._head_pooling_type == 'mean':\n",
    "            node_message_passing = node_message_passing.mean(dim=-3)\n",
    "            edge_message_passing = edge_message_passing.mean(dim=-3)\n",
    "\n",
    "        return node_message_passing, edge_message_passing\n",
    "\n",
    "\n",
    "class MutualAttentionTransformerLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        node_out_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        edge_out_feats: int,\n",
    "        num_heads: int,\n",
    "        short_residual: bool,\n",
    "        long_residual: bool,\n",
    "        dropout_probability: float,\n",
    "        message_aggregation_type: str,\n",
    "        head_pooling_type: str,\n",
    "        normalization_type: str,\n",
    "        linear_projection_activation: str = None,\n",
    "        linear_embedding_activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._long_residual = long_residual\n",
    "        self._mutual_multi_attention_head = MutualMultiAttentionHead(\n",
    "            node_in_feats,\n",
    "            edge_in_feats,\n",
    "            num_heads,\n",
    "            short_residual,\n",
    "            dropout_probability,\n",
    "            message_aggregation_type,\n",
    "            head_pooling_type,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "        self._node_linear_embedding = LinearLayer(\n",
    "            node_in_feats, node_out_feats, linear_embedding_activation)\n",
    "        self._edge_linear_embedding = LinearLayer(\n",
    "            edge_in_feats, edge_out_feats, linear_embedding_activation)\n",
    "\n",
    "        if normalization_type == 'layer':\n",
    "            self._node_normalization_1 = nn.LayerNorm(node_in_feats)\n",
    "            self._node_normalization_2 = nn.LayerNorm(node_out_feats)\n",
    "\n",
    "            self._edge_normalization_1 = nn.LayerNorm(edge_in_feats)\n",
    "            self._edge_normalization_2 = nn.LayerNorm(edge_out_feats)\n",
    "        elif normalization_type == 'batch':\n",
    "            self._node_normalization_1 = nn.BatchNorm1d(node_in_feats)\n",
    "            self._node_normalization_2 = nn.BatchNorm1d(node_out_feats)\n",
    "\n",
    "            self._edge_normalization_1 = nn.BatchNorm1d(edge_in_feats)\n",
    "            self._edge_normalization_2 = nn.BatchNorm1d(edge_out_feats)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        lg: dgl.DGLGraph,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        node_embedding, edge_embedding = self._mutual_multi_attention_head(\n",
    "            g, lg, node_inputs, edge_inputs)\n",
    "\n",
    "        if self._long_residual:\n",
    "            node_embedding += node_inputs\n",
    "            edge_embedding += edge_inputs\n",
    "\n",
    "        node_embedding = self._node_normalization_1(node_embedding)\n",
    "        edge_embedding = self._edge_normalization_1(edge_embedding)\n",
    "\n",
    "        node_embedding = self._node_linear_embedding(node_embedding)\n",
    "        edge_embedding = self._edge_linear_embedding(edge_embedding)\n",
    "\n",
    "        node_embedding = self._node_normalization_2(node_embedding)\n",
    "        edge_embedding = self._edge_normalization_2(edge_embedding)\n",
    "\n",
    "        return node_embedding, edge_embedding\n",
    "\n",
    "\n",
    "class GraphMutualAttentionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        node_hidden_feats: int,\n",
    "        node_out_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        edge_hidden_feats: int,\n",
    "        edge_out_feats: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        short_residual: bool,\n",
    "        long_residual: bool,\n",
    "        dropout_probability: float,\n",
    "        message_aggregation_type: str,\n",
    "        head_pooling_type: str,\n",
    "        readout_pooling_type: str,\n",
    "        normalization_type: str,\n",
    "        linear_projection_activation: str = None,\n",
    "        linear_embedding_activation: str = None,\n",
    "        bilinear_readout_activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._node_out_feats = node_out_feats\n",
    "        self._edge_out_feats = edge_out_feats\n",
    "        self._num_layers = num_layers\n",
    "        self._readout_pooling_type = readout_pooling_type\n",
    "        self._transformer_layers = self._create_transformer_layers(\n",
    "            node_in_feats,\n",
    "            node_hidden_feats,\n",
    "            node_out_feats,\n",
    "            edge_in_feats,\n",
    "            edge_hidden_feats,\n",
    "            edge_out_feats,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            short_residual,\n",
    "            long_residual,\n",
    "            dropout_probability,\n",
    "            message_aggregation_type,\n",
    "            head_pooling_type,\n",
    "            normalization_type,\n",
    "            linear_projection_activation,\n",
    "            linear_embedding_activation,\n",
    "        )\n",
    "        self._bilinear_readout = BilinearReadoutLayer(\n",
    "            node_out_feats, edge_out_feats, 1, bilinear_readout_activation)\n",
    "\n",
    "    def _create_transformer_layers(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        node_hidden_feats: int,\n",
    "        node_out_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        edge_hidden_feats: int,\n",
    "        edge_out_feats: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        short_residual: bool,\n",
    "        long_residual: bool,\n",
    "        dropout_probability: float,\n",
    "        message_aggregation_type: str,\n",
    "        head_pooling_type: str,\n",
    "        normalization_type: str,\n",
    "        linear_projection_activation: str = None,\n",
    "        linear_embedding_activation: str = None,\n",
    "    ) -> nn.ModuleList:\n",
    "        transformer_layers = nn.ModuleList()\n",
    "\n",
    "        if num_layers > 1:\n",
    "            transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                node_in_feats,\n",
    "                node_hidden_feats,\n",
    "                edge_in_feats,\n",
    "                edge_hidden_feats,\n",
    "                num_heads,\n",
    "                short_residual,\n",
    "                long_residual,\n",
    "                dropout_probability,\n",
    "                message_aggregation_type,\n",
    "                head_pooling_type,\n",
    "                normalization_type,\n",
    "                linear_projection_activation,\n",
    "                linear_embedding_activation,\n",
    "            ))\n",
    "\n",
    "            for _ in range(num_layers - 2):\n",
    "                transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                    node_hidden_feats,\n",
    "                    node_hidden_feats,\n",
    "                    edge_hidden_feats,\n",
    "                    edge_hidden_feats,\n",
    "                    num_heads,\n",
    "                    short_residual,\n",
    "                    long_residual,\n",
    "                    dropout_probability,\n",
    "                    message_aggregation_type,\n",
    "                    head_pooling_type,\n",
    "                    normalization_type,\n",
    "                    linear_projection_activation,\n",
    "                    linear_embedding_activation,\n",
    "                ))\n",
    "\n",
    "            transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                node_hidden_feats,\n",
    "                node_out_feats,\n",
    "                edge_hidden_feats,\n",
    "                edge_out_feats,\n",
    "                num_heads,\n",
    "                short_residual,\n",
    "                long_residual,\n",
    "                dropout_probability,\n",
    "                message_aggregation_type,\n",
    "                head_pooling_type,\n",
    "                normalization_type,\n",
    "                linear_projection_activation,\n",
    "                linear_embedding_activation,\n",
    "            ))\n",
    "        else:\n",
    "            transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                node_in_feats,\n",
    "                node_out_feats,\n",
    "                edge_in_feats,\n",
    "                edge_out_feats,\n",
    "                num_heads,\n",
    "                short_residual,\n",
    "                long_residual,\n",
    "                dropout_probability,\n",
    "                message_aggregation_type,\n",
    "                head_pooling_type,\n",
    "                normalization_type,\n",
    "                linear_projection_activation,\n",
    "                linear_embedding_activation,\n",
    "            ))\n",
    "\n",
    "        return transformer_layers\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        lg: dgl.DGLGraph,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        node_embedding = node_inputs\n",
    "        edge_embedding = edge_inputs\n",
    "\n",
    "        for transformer_layer in self._transformer_layers:\n",
    "            node_embedding, edge_embedding = transformer_layer(\n",
    "                g, lg, node_inputs, edge_inputs)\n",
    "\n",
    "        if self._readout_pooling_type == 'sum':\n",
    "            node_embedding = node_embedding.sum(dim=-2)\n",
    "            edge_embedding = edge_embedding.sum(dim=-2)\n",
    "        elif self._readout_pooling_type == 'mean':\n",
    "            node_embedding = node_embedding.mean(dim=-2)\n",
    "            edge_embedding = edge_embedding.mean(dim=-2)\n",
    "\n",
    "        readout = self._bilinear_readout(node_embedding, edge_embedding)\n",
    "\n",
    "        return readout\n"
   ]
  },
  {
   "source": [
    "# Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, dataset, num_epochs: int) -> None:\n",
    "    split_dict = dataset.get_idx_split()\n",
    "\n",
    "    train_idx = split_dict['train']\n",
    "    valid_idx = split_dict['valid']\n",
    "    test_idx = split_dict['test']\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        start = default_timer()\n",
    "\n",
    "        train_loss = 0\n",
    "        validation_loss = 0\n",
    "\n",
    "        # training\n",
    "        for i in train_idx:\n",
    "            g = dataset[i][0]\n",
    "            ground_truth = dataset[i][1]\n",
    "\n",
    "            g = dgl.to_homogeneous(g, ndata=g.ndata, edata=g.edata, store_type=False)\n",
    "            g = dgl.add_self_loop(g)\n",
    "            lg = dgl.line_graph(g, backtracking=False)\n",
    "\n",
    "            node_inputs = g.ndata['feat'].float()\n",
    "            edge_inputs = g.edata['feat'].float()\n",
    "\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(g, lg, node_inputs, edge_inputs)\n",
    "\n",
    "            train_loss += F.l1_loss(outputs[0], ground_truth)\n",
    "\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # validation\n",
    "        for i in valid_idx:\n",
    "            g = dataset[i][0]\n",
    "            ground_truth = dataset[i][1]\n",
    "\n",
    "            g = dgl.to_homogeneous(g, ndata=g.ndata, edata=g.edata, store_type=False)\n",
    "            g = dgl.add_self_loop(g)\n",
    "            lg = dgl.line_graph(g, backtracking=False)\n",
    "\n",
    "            node_inputs = g.ndata['feat'].float()\n",
    "            edge_inputs = g.edata['feat'].float()\n",
    "\n",
    "            model.eval()\n",
    "\n",
    "            outputs = model(g, lg, node_inputs, edge_inputs)\n",
    "\n",
    "            validation_loss += F.l1_loss(outputs[0], ground_truth)\n",
    "        \n",
    "        stop = default_timer()\n",
    "\n",
    "        print(f'Epoch: {epoch} Train loss: {train_loss} Validation loss: {validation_loss} Epoch time: {stop - start}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "linalg_inv_lu: U(45,45) is zero, singular U.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-31bb6d7b2335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-ae06d728a7d4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataset, num_epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/ogb-lsc-env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-934084b7a9b8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, lg, node_inputs, edge_inputs)\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtransformer_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformer_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m             node_embedding, edge_embedding = transformer_layer(\n\u001b[0m\u001b[1;32m    469\u001b[0m                 g, lg, node_inputs, edge_inputs)\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/ogb-lsc-env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-934084b7a9b8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, lg, node_inputs, edge_inputs)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0medge_inputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m     ) -> Tuple[torch.Tensor, torch.Tensor]:\n\u001b[0;32m--> 301\u001b[0;31m         node_embedding, edge_embedding = self._mutual_multi_attention_head(\n\u001b[0m\u001b[1;32m    302\u001b[0m             g, lg, node_inputs, edge_inputs)\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/ogb-lsc-env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-934084b7a9b8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, lg, node_inputs, edge_inputs)\u001b[0m\n\u001b[1;32m    229\u001b[0m         node_message_passing = self._calculate_message_passing(\n\u001b[1;32m    230\u001b[0m             g, node_value, node_attention_projection)\n\u001b[0;32m--> 231\u001b[0;31m         edge_message_passing = self._calculate_message_passing(\n\u001b[0m\u001b[1;32m    232\u001b[0m             lg, edge_value, edge_attention_projection)\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-934084b7a9b8>\u001b[0m in \u001b[0;36m_calculate_message_passing\u001b[0;34m(self, g, value, attention_projection)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_projection\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_aggregation_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mdegree_inv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_degrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdegree_inv\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mattention_projection\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0madjacency\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: linalg_inv_lu: U(45,45) is zero, singular U."
     ]
    }
   ],
   "source": [
    "model = GraphMutualAttentionTransformer(\n",
    "    node_in_feats=9,\n",
    "    node_hidden_feats=9,\n",
    "    node_out_feats=9,\n",
    "    edge_in_feats=3,\n",
    "    edge_hidden_feats=3,\n",
    "    edge_out_feats=3,\n",
    "    num_layers=3,\n",
    "    num_heads=16,\n",
    "    short_residual=True,\n",
    "    long_residual=True,\n",
    "    dropout_probability=0.01,\n",
    "    message_aggregation_type='sum',\n",
    "    head_pooling_type='sum',\n",
    "    readout_pooling_type='mean',\n",
    "    normalization_type='layer',\n",
    "    linear_projection_activation='relu',\n",
    "    linear_embedding_activation='relu',\n",
    "    bilinear_readout_activation='softplus',\n",
    ")\n",
    "\n",
    "train(model, dataset, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}