{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd04884a46182a36c52786c58ae747f94ef8bde10a8978cfcd06107010b24cc677d",
   "display_name": "Python 3.8.8 64-bit ('ogb-lsc-env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using backend: pytorch\n",
      "WARNING:root:The OGB package is out of date. Your version is 1.3.0, while the latest version is 1.3.1.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f59c40bad70>"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import math\n",
    "from timeit import default_timer\n",
    "from typing import Union, Tuple\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from ogb.graphproppred import DglGraphPropPredDataset, Evaluator\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import trange, tqdm\n",
    "\n",
    "from torch.profiler.profiler import tensorboard_trace_handler\n",
    "\n",
    "torch.manual_seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 41127/41127 [00:30<00:00, 1343.03it/s]\n"
     ]
    }
   ],
   "source": [
    "class ProcessedMolhiv(dgl.data.DGLDataset):\n",
    "    def __init__(self, ogb_dataset: dgl.data.DGLDataset, normalize: bool = False) -> None:\n",
    "        self._ogb_dataset = ogb_dataset\n",
    "        self._normalize = normalize\n",
    "        self.graphs = []\n",
    "        self.line_graphs = []\n",
    "        self.labels = []\n",
    "        super().__init__(name='ProcessedMolhiv')\n",
    "\n",
    "    def process(self):\n",
    "        max_node = 0\n",
    "        min_node = 0\n",
    "        max_edge = 0\n",
    "        min_edge = 0\n",
    "\n",
    "        for i in trange(len(self._ogb_dataset)):\n",
    "            g = self._ogb_dataset[i][0].add_self_loop()\n",
    "            lg = dgl.line_graph(g, backtracking=False).add_self_loop()\n",
    "\n",
    "            g.ndata['feat'] = g.ndata['feat'].float()\n",
    "            g.edata['feat'] = g.edata['feat'].float()\n",
    "\n",
    "            if self._normalize:\n",
    "                g.ndata['feat'] /= 91\n",
    "                g.edata['feat'] /= 3\n",
    "\n",
    "            # if g.ndata['feat'].max() > max_node:\n",
    "            #     max_node = g.ndata['feat'].max()\n",
    "            \n",
    "            # if g.ndata['feat'].min() < min_node:\n",
    "            #     min_node = g.ndata['feat'].min()\n",
    "\n",
    "            # if g.edata['feat'].max() > max_edge:\n",
    "            #     max_edge = g.edata['feat'].max()\n",
    "            \n",
    "            # if g.edata['feat'].min() < min_edge:\n",
    "            #     min_edge = g.edata['feat'].min()\n",
    "\n",
    "            self.graphs.append(g)\n",
    "            self.line_graphs.append(lg)\n",
    "            self.labels.append(self._ogb_dataset[i][1])\n",
    "\n",
    "        # print(f'max_node: {max_node}')\n",
    "        # print(f'min_node: {min_node}')\n",
    "        # print(f'max_edge: {max_edge}')\n",
    "        # print(f'min_edge: {min_edge}')\n",
    "\n",
    "    def __getitem__(self, idx: Union[int, torch.Tensor]):\n",
    "        if isinstance(idx, int):\n",
    "            return self.graphs[idx], self.line_graphs[idx], self.labels[idx]\n",
    "        elif torch.is_tensor(idx) and idx.dtype == torch.long:\n",
    "            if idx.dim() == 0:\n",
    "                return self.graphs[idx], self.line_graphs[idx], self.labels[idx]\n",
    "            elif idx.dim() == 1:\n",
    "                return dgl.data.utils.Subset(self, idx.cpu())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "dataset = DglGraphPropPredDataset(root='/home/ksadowski/datasets', name='ogbg-molhiv')\n",
    "processed_dataset = ProcessedMolhiv(dataset, normalize=True)\n",
    "\n",
    "split_idx = dataset.get_idx_split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train samples: 515\nVal samples: 65\nTest samples: 65\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "train_dataloader = dgl.dataloading.pytorch.GraphDataLoader(\n",
    "    processed_dataset[split_idx['train']],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "val_dataloader = dgl.dataloading.pytorch.GraphDataLoader(\n",
    "    processed_dataset[split_idx['valid']],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "test_dataloader = dgl.dataloading.pytorch.GraphDataLoader(\n",
    "    processed_dataset[split_idx['test']],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "print(f'Train samples: {len(train_dataloader)}')\n",
    "print(f'Val samples: {len(val_dataloader)}')\n",
    "print(f'Test samples: {len(test_dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feats: int,\n",
    "        out_feats: int,\n",
    "        activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._linear = nn.Linear(in_feats, out_feats)\n",
    "        self._activation = activation\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        x = self._linear(inputs)\n",
    "\n",
    "        if self._activation == 'relu':\n",
    "            x = F.relu(x)\n",
    "        elif self._activation == 'relu6':\n",
    "            x = F.relu6(x)\n",
    "        elif self._activation == 'leaky_relu':\n",
    "            x = F.leaky_relu(x)\n",
    "        elif self._activation == 'elu':\n",
    "            x = F.elu(x)\n",
    "        elif self._activation == 'selu':\n",
    "            x = F.selu(x)\n",
    "        elif self._activation == 'celu':\n",
    "            x = F.celu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BilinearReadoutLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        out_feats: int,\n",
    "        activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._bilinear = nn.Bilinear(node_in_feats, edge_in_feats, out_feats)\n",
    "        self._activation = activation\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> float:\n",
    "        x = self._bilinear(node_inputs, edge_inputs)\n",
    "\n",
    "        if self._activation == 'relu':\n",
    "            x = F.relu(x)\n",
    "        elif self._activation == 'softplus':\n",
    "            x = F.softplus(x)\n",
    "        elif self._activation == 'sigmoid':\n",
    "            x = F.sigmoid(x)\n",
    "        elif self._activation == 'softmax':\n",
    "            x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MutualMultiAttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        num_heads: int,\n",
    "        dropout_probability: float,\n",
    "        message_aggregation_type: str,\n",
    "        head_pooling_type: str,\n",
    "        linear_projection_activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._node_in_feats = node_in_feats\n",
    "        self._edge_in_feats = edge_in_feats\n",
    "        self._num_heads = num_heads\n",
    "        self._message_aggregation_type = message_aggregation_type\n",
    "        self._head_pooling_type = head_pooling_type\n",
    "        self._device = nn.Parameter(torch.empty(0))\n",
    "        self._node_key_linear = LinearLayer(\n",
    "            node_in_feats, num_heads, linear_projection_activation)\n",
    "        self._node_value_linear = LinearLayer(\n",
    "            node_in_feats,\n",
    "            num_heads * node_in_feats,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "        self._edge_key_linear = LinearLayer(\n",
    "            edge_in_feats, num_heads, linear_projection_activation)\n",
    "        self._edge_value_linear = LinearLayer(\n",
    "            edge_in_feats,\n",
    "            num_heads * edge_in_feats,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "        self._node_dropout = nn.Dropout(dropout_probability)\n",
    "        self._edge_dropout = nn.Dropout(dropout_probability)\n",
    "\n",
    "    def _calculate_self_attention(\n",
    "        self,\n",
    "        key: torch.Tensor,\n",
    "        in_feats: int,\n",
    "    ) -> torch.Tensor:\n",
    "        x = key / math.sqrt(in_feats)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    @torch.jit.script\n",
    "    def _node_attention_script(\n",
    "        edge_self_attention: torch.Tensor, \n",
    "        source_nodes: torch.Tensor, \n",
    "        destination_nodes: torch.Tensor, \n",
    "        num_heads: int, \n",
    "        num_nodes: int, \n",
    "        num_edges: int\n",
    "    ) -> torch.Tensor:\n",
    "        attention_projection = torch.zeros(\n",
    "            [num_heads, num_nodes, num_nodes],\n",
    "            dtype=torch.float32,\n",
    "            device='cpu',\n",
    "        )\n",
    "\n",
    "        for edge in range(num_edges):\n",
    "            source = source_nodes[edge]\n",
    "            destination = destination_nodes[edge]\n",
    "\n",
    "            for head in range(num_heads):\n",
    "                attention_score = edge_self_attention[head][edge]\n",
    "\n",
    "                attention_projection[head][source][destination] = attention_score\n",
    "\n",
    "        return attention_projection\n",
    "\n",
    "    def _create_node_attention_projection(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        edge_self_attention: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        edges = g.edges()\n",
    "\n",
    "        node_attention_projection = self._node_attention_script(\n",
    "            edge_self_attention,\n",
    "            edges[0],\n",
    "            edges[1],\n",
    "            self._num_heads,\n",
    "            g.num_nodes(),\n",
    "            g.num_edges(),\n",
    "        )\n",
    "\n",
    "        return node_attention_projection\n",
    "\n",
    "    @torch.jit.script\n",
    "    def _edge_attention_script(\n",
    "        node_self_attention: torch.Tensor,\n",
    "        source_lg_nodes: torch.Tensor,\n",
    "        destination_lg_nodes: torch.Tensor,\n",
    "        destination_g_nodes: torch.Tensor,\n",
    "        num_heads: int, \n",
    "        num_g_edges: int,\n",
    "        num_lg_edges: int,\n",
    "    ):\n",
    "        attention_projection = torch.zeros(\n",
    "            [num_heads, num_g_edges, num_g_edges],\n",
    "            dtype=torch.float32,\n",
    "            device='cpu',\n",
    "        )\n",
    "\n",
    "        for lg_edge in range(num_lg_edges):\n",
    "            source = source_lg_nodes[lg_edge]\n",
    "            destination = destination_lg_nodes[lg_edge]\n",
    "\n",
    "            connecting_g_node = destination_g_nodes[source]\n",
    "\n",
    "            for head in range(num_heads):\n",
    "                attention_score = node_self_attention[head][connecting_g_node]\n",
    "\n",
    "                attention_projection[head][source][destination] = attention_score\n",
    "\n",
    "        return attention_projection\n",
    "\n",
    "    def _create_edge_attention_projection(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        lg: dgl.DGLGraph,\n",
    "        node_self_attention: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "\n",
    "        edge_attention_projection = self._edge_attention_script(\n",
    "            node_self_attention,\n",
    "            lg.edges()[0],\n",
    "            lg.edges()[1],\n",
    "            g.edges()[1],\n",
    "            self._num_heads, \n",
    "            g.num_edges(),\n",
    "            lg.num_edges(),\n",
    "            )\n",
    "\n",
    "        return edge_attention_projection\n",
    "\n",
    "    def _calculate_message_passing(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        value: torch.Tensor,\n",
    "        attention_projection: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        if self._message_aggregation_type == 'sum':\n",
    "            x = attention_projection\n",
    "        elif self._message_aggregation_type == 'mean':\n",
    "            degree_inv = torch.linalg.inv(torch.diag(g.in_degrees().float()))\n",
    "\n",
    "            x = degree_inv @ attention_projection\n",
    "        elif self._message_aggregation_type == 'gcn':\n",
    "            adjacency = g.adj(ctx=self._device.device).to_dense()\n",
    "\n",
    "            degree_inv_sqrt = torch.sqrt(torch.linalg.inv(\n",
    "                torch.diag(g.in_degrees().float())))\n",
    "            adjacency_inv_sqrt = torch.sqrt(torch.linalg.inv(adjacency))\n",
    "\n",
    "            x = degree_inv_sqrt @ attention_projection * \\\n",
    "                adjacency_inv_sqrt @ degree_inv_sqrt\n",
    "\n",
    "        message_passing = x @ value\n",
    "\n",
    "        return message_passing\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        lg: dgl.DGLGraph,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        node_key = self._node_key_linear(node_inputs)\n",
    "        node_key = node_key.view(self._num_heads, -1, 1)\n",
    "        node_value = self._node_value_linear(node_inputs)\n",
    "        node_value = node_value.view(self._num_heads, -1, self._node_in_feats)\n",
    "\n",
    "        edge_key = self._edge_key_linear(edge_inputs)\n",
    "        edge_key = edge_key.view(self._num_heads, -1, 1)\n",
    "        edge_value = self._edge_value_linear(edge_inputs)\n",
    "        edge_value = edge_value.view(self._num_heads, -1, self._edge_in_feats)\n",
    "\n",
    "        node_self_attention = self._calculate_self_attention(\n",
    "            node_key, self._node_in_feats)\n",
    "        edge_self_attention = self._calculate_self_attention(\n",
    "            edge_key, self._edge_in_feats)\n",
    "\n",
    "        node_attention_projection = self._create_node_attention_projection(\n",
    "            g, edge_self_attention)\n",
    "        edge_attention_projection = self._create_edge_attention_projection(\n",
    "            g, lg, node_self_attention)\n",
    "\n",
    "        node_message_passing = self._calculate_message_passing(\n",
    "            g, node_value, node_attention_projection)\n",
    "        edge_message_passing = self._calculate_message_passing(\n",
    "            lg, edge_value, edge_attention_projection)\n",
    "\n",
    "        node_message_passing = self._node_dropout(node_message_passing)\n",
    "        edge_message_passing = self._edge_dropout(edge_message_passing)\n",
    "\n",
    "        if self._head_pooling_type == 'sum':\n",
    "            node_message_passing = node_message_passing.sum(dim=-3)\n",
    "            edge_message_passing = edge_message_passing.sum(dim=-3)\n",
    "        elif self._head_pooling_type == 'mean':\n",
    "            node_message_passing = node_message_passing.mean(dim=-3)\n",
    "            edge_message_passing = edge_message_passing.mean(dim=-3)\n",
    "\n",
    "        return node_message_passing, edge_message_passing\n",
    "\n",
    "\n",
    "class MutualAttentionTransformerLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        node_out_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        edge_out_feats: int,\n",
    "        num_heads: int,\n",
    "        long_residual: bool,\n",
    "        dropout_probability: float,\n",
    "        message_aggregation_type: str,\n",
    "        head_pooling_type: str,\n",
    "        normalization_type: str,\n",
    "        linear_projection_activation: str = None,\n",
    "        linear_embedding_activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._long_residual = long_residual\n",
    "        self._mutual_multi_attention_head = MutualMultiAttentionHead(\n",
    "            node_in_feats,\n",
    "            edge_in_feats,\n",
    "            num_heads,\n",
    "            dropout_probability,\n",
    "            message_aggregation_type,\n",
    "            head_pooling_type,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "        self._node_linear_embedding = LinearLayer(\n",
    "            node_in_feats, node_out_feats, linear_embedding_activation)\n",
    "        self._edge_linear_embedding = LinearLayer(\n",
    "            edge_in_feats, edge_out_feats, linear_embedding_activation)\n",
    "\n",
    "        if normalization_type == 'layer':\n",
    "            self._node_normalization_1 = nn.LayerNorm(node_in_feats)\n",
    "            self._node_normalization_2 = nn.LayerNorm(node_out_feats)\n",
    "\n",
    "            self._edge_normalization_1 = nn.LayerNorm(edge_in_feats)\n",
    "            self._edge_normalization_2 = nn.LayerNorm(edge_out_feats)\n",
    "        elif normalization_type == 'batch':\n",
    "            self._node_normalization_1 = nn.BatchNorm1d(node_in_feats)\n",
    "            self._node_normalization_2 = nn.BatchNorm1d(node_out_feats)\n",
    "\n",
    "            self._edge_normalization_1 = nn.BatchNorm1d(edge_in_feats)\n",
    "            self._edge_normalization_2 = nn.BatchNorm1d(edge_out_feats)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        lg: dgl.DGLGraph,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        node_embedding, edge_embedding = self._mutual_multi_attention_head(\n",
    "            g, lg, node_inputs, edge_inputs)\n",
    "\n",
    "        if self._long_residual:\n",
    "            node_embedding += node_inputs\n",
    "            edge_embedding += edge_inputs\n",
    "\n",
    "        node_embedding = self._node_normalization_1(node_embedding)\n",
    "        edge_embedding = self._edge_normalization_1(edge_embedding)\n",
    "\n",
    "        node_embedding = self._node_linear_embedding(node_embedding)\n",
    "        edge_embedding = self._edge_linear_embedding(edge_embedding)\n",
    "\n",
    "        node_embedding = self._node_normalization_2(node_embedding)\n",
    "        edge_embedding = self._edge_normalization_2(edge_embedding)\n",
    "\n",
    "        return node_embedding, edge_embedding\n",
    "\n",
    "\n",
    "class GraphMutualAttentionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        node_hidden_feats: int,\n",
    "        node_out_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        edge_hidden_feats: int,\n",
    "        edge_out_feats: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        long_residual: bool,\n",
    "        dropout_probability: float,\n",
    "        message_aggregation_type: str,\n",
    "        head_pooling_type: str,\n",
    "        readout_pooling_type: str,\n",
    "        normalization_type: str,\n",
    "        linear_projection_activation: str = None,\n",
    "        linear_embedding_activation: str = None,\n",
    "        bilinear_readout_activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._node_out_feats = node_out_feats\n",
    "        self._edge_out_feats = edge_out_feats\n",
    "        self._num_layers = num_layers\n",
    "        self._transformer_layers = self._create_transformer_layers(\n",
    "            node_in_feats,\n",
    "            node_hidden_feats,\n",
    "            node_out_feats,\n",
    "            edge_in_feats,\n",
    "            edge_hidden_feats,\n",
    "            edge_out_feats,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            long_residual,\n",
    "            dropout_probability,\n",
    "            message_aggregation_type,\n",
    "            head_pooling_type,\n",
    "            normalization_type,\n",
    "            linear_projection_activation,\n",
    "            linear_embedding_activation,\n",
    "        )\n",
    "        self._bilinear_readout = BilinearReadoutLayer(\n",
    "            node_out_feats, edge_out_feats, 1, bilinear_readout_activation)\n",
    "\n",
    "        if readout_pooling_type == 'sum':\n",
    "            self._readout_pooling = dgl.nn.pytorch.SumPooling()\n",
    "        elif readout_pooling_type == 'mean':\n",
    "            self._readout_pooling = dgl.nn.pytorch.AvgPooling()\n",
    "        elif readout_pooling_type == 'attention':\n",
    "            pass\n",
    "\n",
    "    def _create_transformer_layers(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        node_hidden_feats: int,\n",
    "        node_out_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        edge_hidden_feats: int,\n",
    "        edge_out_feats: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        long_residual: bool,\n",
    "        dropout_probability: float,\n",
    "        message_aggregation_type: str,\n",
    "        head_pooling_type: str,\n",
    "        normalization_type: str,\n",
    "        linear_projection_activation: str = None,\n",
    "        linear_embedding_activation: str = None,\n",
    "    ) -> nn.ModuleList:\n",
    "        transformer_layers = nn.ModuleList()\n",
    "\n",
    "        if num_layers > 1:\n",
    "            transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                node_in_feats,\n",
    "                node_hidden_feats,\n",
    "                edge_in_feats,\n",
    "                edge_hidden_feats,\n",
    "                num_heads,\n",
    "                long_residual,\n",
    "                dropout_probability,\n",
    "                message_aggregation_type,\n",
    "                head_pooling_type,\n",
    "                normalization_type,\n",
    "                linear_projection_activation,\n",
    "                linear_embedding_activation,\n",
    "            ))\n",
    "\n",
    "            for _ in range(num_layers - 2):\n",
    "                transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                    node_hidden_feats,\n",
    "                    node_hidden_feats,\n",
    "                    edge_hidden_feats,\n",
    "                    edge_hidden_feats,\n",
    "                    num_heads,\n",
    "                    long_residual,\n",
    "                    dropout_probability,\n",
    "                    message_aggregation_type,\n",
    "                    head_pooling_type,\n",
    "                    normalization_type,\n",
    "                    linear_projection_activation,\n",
    "                    linear_embedding_activation,\n",
    "                ))\n",
    "\n",
    "            transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                node_hidden_feats,\n",
    "                node_out_feats,\n",
    "                edge_hidden_feats,\n",
    "                edge_out_feats,\n",
    "                num_heads,\n",
    "                long_residual,\n",
    "                dropout_probability,\n",
    "                message_aggregation_type,\n",
    "                head_pooling_type,\n",
    "                normalization_type,\n",
    "                linear_projection_activation,\n",
    "                linear_embedding_activation,\n",
    "            ))\n",
    "        else:\n",
    "            transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                node_in_feats,\n",
    "                node_out_feats,\n",
    "                edge_in_feats,\n",
    "                edge_out_feats,\n",
    "                num_heads,\n",
    "                long_residual,\n",
    "                dropout_probability,\n",
    "                message_aggregation_type,\n",
    "                head_pooling_type,\n",
    "                normalization_type,\n",
    "                linear_projection_activation,\n",
    "                linear_embedding_activation,\n",
    "            ))\n",
    "\n",
    "        return transformer_layers\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        lg: dgl.DGLGraph,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        node_embedding = node_inputs\n",
    "        edge_embedding = edge_inputs\n",
    "\n",
    "        for transformer_layer in self._transformer_layers:\n",
    "            node_embedding, edge_embedding = transformer_layer(\n",
    "                g, lg, node_embedding, edge_embedding)\n",
    "\n",
    "        node_embedding = self._readout_pooling(g, node_embedding)\n",
    "        edge_embedding = self._readout_pooling(lg, edge_embedding)\n",
    "\n",
    "        readout = self._bilinear_readout(node_embedding, edge_embedding)\n",
    "\n",
    "        return readout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, device: str, dataloader: DataLoader) -> None:\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    loss_accum = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # with torch.profiler.profile(\n",
    "    #     schedule=torch.profiler.schedule(\n",
    "    #         wait=2,\n",
    "    #         warmup=2,\n",
    "    #         active=6,\n",
    "    #         repeat=1,\n",
    "    #     ),\n",
    "    #     activities=[\n",
    "    #         torch.profiler.ProfilerActivity.CPU,\n",
    "    #         # torch.profiler.ProfilerActivity.CUDA,\n",
    "    #     ],\n",
    "    #     on_trace_ready=tensorboard_trace_handler('/home/ksadowski/projects/data_science/gmat/ogb_molhiv/profiler_logs'),\n",
    "    # ) as profiler:\n",
    "    # with tqdm(total=len(dataloader), desc='Batch steps') as pbar:\n",
    "    for batched_g, batched_lg, labels in dataloader:\n",
    "        batched_g = batched_g.to(device)\n",
    "        batched_lg = batched_lg.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        node_inputs = batched_g.ndata.pop('feat')\n",
    "        edge_inputs = batched_g.edata.pop('feat')\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = model(batched_g, batched_lg, node_inputs, edge_inputs)\n",
    "\n",
    "        loss = F.binary_cross_entropy_with_logits(pred.to(torch.float32), labels.to(torch.float32))\n",
    "        loss_accum += loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # profiler.step()\n",
    "\n",
    "            # pbar.update(1)\n",
    "\n",
    "    return loss_accum / len(dataloader)\n",
    "\n",
    "\n",
    "def eval(model: nn.Module, device: str, dataloader: DataLoader, evaluator: Evaluator):\n",
    "    model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for batched_g, batched_lg, labels in dataloader:\n",
    "        batched_g = batched_g.to(device)\n",
    "        batched_lg = batched_lg.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        node_inputs = batched_g.ndata.pop('feat')\n",
    "        edge_inputs = batched_g.edata.pop('feat')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(batched_g, batched_lg, node_inputs, edge_inputs)\n",
    "\n",
    "        y_true.append(labels.view(pred.shape).detach().cpu())\n",
    "        y_pred.append(pred.detach().cpu())\n",
    "\n",
    "    y_true = torch.cat(y_true, dim = 0).numpy()\n",
    "    y_pred = torch.cat(y_pred, dim = 0).numpy()\n",
    "\n",
    "    input_dict = {\"y_true\": y_true, \"y_pred\": y_pred}\n",
    "\n",
    "    return evaluator.eval(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = processed_dataset[0][0]\n",
    "lg = processed_dataset[0][1]\n",
    "\n",
    "node_inputs = g.ndata['feat']\n",
    "edge_inputs = g.edata['feat']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = GraphMutualAttentionTransformer(\n",
    "    node_in_feats=9,\n",
    "    node_hidden_feats=9,\n",
    "    node_out_feats=9,\n",
    "    edge_in_feats=3,\n",
    "    edge_hidden_feats=3,\n",
    "    edge_out_feats=3,\n",
    "    num_layers=3,\n",
    "    num_heads=4,\n",
    "    # short_residual=True,\n",
    "    long_residual=True,\n",
    "    dropout_probability=0.01,\n",
    "    message_aggregation_type='sum',\n",
    "    head_pooling_type='mean',\n",
    "    readout_pooling_type='mean',\n",
    "    normalization_type='batch',\n",
    "    linear_projection_activation='relu',\n",
    "    linear_embedding_activation='relu',\n",
    "    # bilinear_readout_activation='softmax',\n",
    ").to(device)\n",
    "\n",
    "# model(g, node_inputs, edge_inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch:   1 Train Loss: 0.3739 ROC AUC // Val: 0.5584 Test: 0.5190 // Epoch Time: 2.10 min.\n",
      "Epoch:   2 Train Loss: 0.1573 ROC AUC // Val: 0.6440 Test: 0.6328 // Epoch Time: 2.10 min.\n",
      "Epoch:   3 Train Loss: 0.1521 ROC AUC // Val: 0.6373 Test: 0.6291 // Epoch Time: 2.11 min.\n",
      "Epoch:   4 Train Loss: 0.1499 ROC AUC // Val: 0.6546 Test: 0.6215 // Epoch Time: 2.11 min.\n",
      "Epoch:   5 Train Loss: 0.1495 ROC AUC // Val: 0.6977 Test: 0.6453 // Epoch Time: 2.12 min.\n",
      "Epoch:   6 Train Loss: 0.1484 ROC AUC // Val: 0.6929 Test: 0.6702 // Epoch Time: 2.10 min.\n",
      "Epoch:   7 Train Loss: 0.1479 ROC AUC // Val: 0.7019 Test: 0.6541 // Epoch Time: 2.12 min.\n",
      "Epoch:   8 Train Loss: 0.1474 ROC AUC // Val: 0.6839 Test: 0.6556 // Epoch Time: 2.13 min.\n",
      "Epoch:   9 Train Loss: 0.1475 ROC AUC // Val: 0.7083 Test: 0.6550 // Epoch Time: 2.11 min.\n",
      "Epoch:  10 Train Loss: 0.1469 ROC AUC // Val: 0.7024 Test: 0.6791 // Epoch Time: 2.10 min.\n",
      "Epoch:  11 Train Loss: 0.1479 ROC AUC // Val: 0.6706 Test: 0.6607 // Epoch Time: 2.12 min.\n",
      "Epoch:  12 Train Loss: 0.1469 ROC AUC // Val: 0.6702 Test: 0.6762 // Epoch Time: 2.12 min.\n",
      "Epoch:  13 Train Loss: 0.1473 ROC AUC // Val: 0.6798 Test: 0.6546 // Epoch Time: 2.12 min.\n",
      "Epoch:  14 Train Loss: 0.1459 ROC AUC // Val: 0.6720 Test: 0.6567 // Epoch Time: 2.12 min.\n",
      "Epoch:  15 Train Loss: 0.1463 ROC AUC // Val: 0.6899 Test: 0.6728 // Epoch Time: 2.12 min.\n",
      "Epoch:  16 Train Loss: 0.1458 ROC AUC // Val: 0.6947 Test: 0.6893 // Epoch Time: 2.12 min.\n",
      "Epoch:  17 Train Loss: 0.1457 ROC AUC // Val: 0.7075 Test: 0.6670 // Epoch Time: 2.12 min.\n",
      "Epoch:  18 Train Loss: 0.1456 ROC AUC // Val: 0.6894 Test: 0.6834 // Epoch Time: 2.12 min.\n",
      "Epoch:  19 Train Loss: 0.1458 ROC AUC // Val: 0.6779 Test: 0.6949 // Epoch Time: 2.13 min.\n",
      "Epoch:  20 Train Loss: 0.1456 ROC AUC // Val: 0.6974 Test: 0.6667 // Epoch Time: 2.13 min.\n",
      "Epoch:  21 Train Loss: 0.1451 ROC AUC // Val: 0.7105 Test: 0.6745 // Epoch Time: 2.13 min.\n",
      "Epoch:  22 Train Loss: 0.1450 ROC AUC // Val: 0.6802 Test: 0.6739 // Epoch Time: 2.13 min.\n",
      "Epoch:  23 Train Loss: 0.1462 ROC AUC // Val: 0.6953 Test: 0.6845 // Epoch Time: 2.13 min.\n",
      "Epoch:  24 Train Loss: 0.1455 ROC AUC // Val: 0.6939 Test: 0.6867 // Epoch Time: 2.13 min.\n",
      "Epoch:  25 Train Loss: 0.1444 ROC AUC // Val: 0.6771 Test: 0.6974 // Epoch Time: 2.13 min.\n",
      "Epoch:  26 Train Loss: 0.1444 ROC AUC // Val: 0.6992 Test: 0.6824 // Epoch Time: 2.14 min.\n",
      "Epoch:  27 Train Loss: 0.1459 ROC AUC // Val: 0.6748 Test: 0.6797 // Epoch Time: 2.13 min.\n",
      "Epoch:  28 Train Loss: 0.1442 ROC AUC // Val: 0.6880 Test: 0.6940 // Epoch Time: 2.14 min.\n",
      "Epoch:  29 Train Loss: 0.1452 ROC AUC // Val: 0.6837 Test: 0.6963 // Epoch Time: 2.13 min.\n",
      "Epoch:  30 Train Loss: 0.1439 ROC AUC // Val: 0.6821 Test: 0.6900 // Epoch Time: 2.13 min.\n",
      "Epoch:  31 Train Loss: 0.1442 ROC AUC // Val: 0.6935 Test: 0.6835 // Epoch Time: 2.14 min.\n",
      "Epoch:  32 Train Loss: 0.1439 ROC AUC // Val: 0.6653 Test: 0.6777 // Epoch Time: 2.12 min.\n",
      "Epoch:  33 Train Loss: 0.1440 ROC AUC // Val: 0.6931 Test: 0.6837 // Epoch Time: 2.13 min.\n",
      "Epoch:  34 Train Loss: 0.1440 ROC AUC // Val: 0.7008 Test: 0.6972 // Epoch Time: 2.13 min.\n",
      "Epoch:  35 Train Loss: 0.1435 ROC AUC // Val: 0.6823 Test: 0.6993 // Epoch Time: 2.13 min.\n",
      "Epoch:  36 Train Loss: 0.1434 ROC AUC // Val: 0.6924 Test: 0.7029 // Epoch Time: 2.13 min.\n",
      "Epoch:  37 Train Loss: 0.1431 ROC AUC // Val: 0.6670 Test: 0.6911 // Epoch Time: 2.13 min.\n",
      "Epoch:  38 Train Loss: 0.1431 ROC AUC // Val: 0.6702 Test: 0.6849 // Epoch Time: 2.13 min.\n",
      "Epoch:  39 Train Loss: 0.1433 ROC AUC // Val: 0.6912 Test: 0.6798 // Epoch Time: 2.14 min.\n",
      "Epoch:  40 Train Loss: 0.1438 ROC AUC // Val: 0.7154 Test: 0.6926 // Epoch Time: 2.13 min.\n",
      "Epoch:  41 Train Loss: 0.1437 ROC AUC // Val: 0.6692 Test: 0.6898 // Epoch Time: 2.13 min.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-72757ae455a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain_perf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# train_perf = eval(model, device, train_loader, evaluator)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-79843ee02a69>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, dataloader)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched_lg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/ogb-lsc-env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0349481b546f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, lg, node_inputs, edge_inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtransformer_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformer_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m             node_embedding, edge_embedding = transformer_layer(\n\u001b[0m\u001b[1;32m    489\u001b[0m                 g, lg, node_embedding, edge_embedding)\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/ogb-lsc-env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0349481b546f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, lg, node_inputs, edge_inputs)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0medge_inputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m     ) -> Tuple[torch.Tensor, torch.Tensor]:\n\u001b[0;32m--> 322\u001b[0;31m         node_embedding, edge_embedding = self._mutual_multi_attention_head(\n\u001b[0m\u001b[1;32m    323\u001b[0m             g, lg, node_inputs, edge_inputs)\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.miniconda3/envs/ogb-lsc-env/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0349481b546f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, g, lg, node_inputs, edge_inputs)\u001b[0m\n\u001b[1;32m    247\u001b[0m         node_attention_projection = self._create_node_attention_projection(\n\u001b[1;32m    248\u001b[0m             g, edge_self_attention)\n\u001b[0;32m--> 249\u001b[0;31m         edge_attention_projection = self._create_edge_attention_projection(\n\u001b[0m\u001b[1;32m    250\u001b[0m             g, lg, node_self_attention)\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-0349481b546f>\u001b[0m in \u001b[0;36m_create_edge_attention_projection\u001b[0;34m(self, g, lg, node_self_attention)\u001b[0m\n\u001b[1;32m    185\u001b[0m     ) -> torch.Tensor:\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         edge_attention_projection = self._edge_attention_script(\n\u001b[0m\u001b[1;32m    188\u001b[0m             \u001b[0mnode_self_attention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m             \u001b[0mlg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "\n",
    "evaluator = Evaluator(name='ogbg-molhiv')\n",
    "\n",
    "for epoch in range(1, 1 + NUM_EPOCHS):\n",
    "    start = default_timer()\n",
    "\n",
    "    train_perf = train(model, device, train_dataloader)\n",
    "\n",
    "    # train_perf = eval(model, device, train_loader, evaluator)\n",
    "    val_perf = eval(model, device, val_dataloader, evaluator)\n",
    "    val_perf = val_perf['rocauc']\n",
    "    test_perf = eval(model, device, test_dataloader, evaluator)\n",
    "    test_perf = test_perf['rocauc']\n",
    "\n",
    "    stop = default_timer()\n",
    "\n",
    "    print(\n",
    "        f'Epoch: {epoch: 3} Train Loss: {train_perf:.4f} '\n",
    "        f'ROC AUC // Val: {val_perf:.4f} Test: {test_perf:.4f} // '\n",
    "        f'Epoch Time: {(stop - start) / 60:.2f} min.'\n",
    "    )"
   ]
  }
 ]
}