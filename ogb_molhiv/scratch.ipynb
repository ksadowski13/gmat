{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd04884a46182a36c52786c58ae747f94ef8bde10a8978cfcd06107010b24cc677d",
   "display_name": "Python 3.8.8 64-bit ('ogb-lsc-env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ogb.graphproppred import DglGraphPropPredDataset\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DglGraphPropPredDataset(root='/home/ksadowski/datasets', name='ogbg-molhiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0278],\n        [0.0278],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0278],\n        [0.0278],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0278],\n        [0.0278],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0278],\n        [0.0278],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243]], grad_fn=<SoftmaxBackward>)\ntensor([[ 5.,  0.,  4.,  5.,  3.,  0.,  2.,  0.,  0.],\n        [ 5.,  0.,  4.,  5.,  2.,  0.,  2.,  0.,  0.],\n        [ 5.,  0.,  3.,  5.,  0.,  0.,  1.,  0.,  1.],\n        [ 7.,  0.,  2.,  6.,  0.,  0.,  1.,  0.,  1.],\n        [28.,  0.,  4.,  2.,  0.,  0.,  5.,  0.,  1.],\n        [ 7.,  0.,  2.,  6.,  0.,  0.,  1.,  0.,  1.],\n        [ 5.,  0.,  3.,  5.,  0.,  0.,  1.,  0.,  1.],\n        [ 5.,  0.,  4.,  5.,  2.,  0.,  2.,  0.,  0.],\n        [ 5.,  0.,  4.,  5.,  3.,  0.,  2.,  0.,  0.],\n        [ 5.,  0.,  4.,  5.,  2.,  0.,  2.,  0.,  1.],\n        [ 7.,  0.,  2.,  6.,  0.,  0.,  1.,  0.,  1.],\n        [ 5.,  0.,  3.,  5.,  0.,  0.,  1.,  0.,  1.],\n        [ 5.,  0.,  4.,  5.,  2.,  0.,  2.,  0.,  0.],\n        [ 5.,  0.,  4.,  5.,  3.,  0.,  2.,  0.,  0.],\n        [ 5.,  0.,  4.,  5.,  2.,  0.,  2.,  0.,  1.],\n        [ 5.,  0.,  3.,  5.,  0.,  0.,  1.,  0.,  1.],\n        [ 5.,  0.,  4.,  5.,  2.,  0.,  2.,  0.,  0.],\n        [ 5.,  0.,  4.,  5.,  3.,  0.,  2.,  0.,  0.],\n        [ 7.,  0.,  2.,  6.,  0.,  0.,  1.,  0.,  1.]])\ntensor([[0.1215, 0.0000, 0.0972, 0.1215, 0.0486, 0.0000, 0.0486, 0.0000, 0.0000],\n        [0.2430, 0.0000, 0.1701, 0.2430, 0.0729, 0.0000, 0.0729, 0.0000, 0.0243],\n        [0.4377, 0.0000, 0.2500, 0.4099, 0.0972, 0.0000, 0.1250, 0.0000, 0.0521],\n        [0.8194, 0.0000, 0.1806, 0.1877, 0.0000, 0.0000, 0.1493, 0.0000, 0.0521],\n        [0.6803, 0.0000, 0.1944, 0.5831, 0.0000, 0.0000, 0.0972, 0.0000, 0.0972],\n        [0.8194, 0.0000, 0.1806, 0.1877, 0.0000, 0.0000, 0.1493, 0.0000, 0.0521],\n        [0.4377, 0.0000, 0.2500, 0.4099, 0.0972, 0.0000, 0.1250, 0.0000, 0.0521],\n        [0.2430, 0.0000, 0.1701, 0.2430, 0.0729, 0.0000, 0.0729, 0.0000, 0.0243],\n        [0.1215, 0.0000, 0.0972, 0.1215, 0.0486, 0.0000, 0.0486, 0.0000, 0.0000],\n        [0.2430, 0.0000, 0.1458, 0.2430, 0.0000, 0.0000, 0.0486, 0.0000, 0.0486],\n        [0.8194, 0.0000, 0.1806, 0.1877, 0.0000, 0.0000, 0.1493, 0.0000, 0.0521],\n        [0.4377, 0.0000, 0.2500, 0.4099, 0.0972, 0.0000, 0.1250, 0.0000, 0.0521],\n        [0.2430, 0.0000, 0.1701, 0.2430, 0.0729, 0.0000, 0.0729, 0.0000, 0.0243],\n        [0.1215, 0.0000, 0.0972, 0.1215, 0.0486, 0.0000, 0.0486, 0.0000, 0.0000],\n        [0.2430, 0.0000, 0.1458, 0.2430, 0.0000, 0.0000, 0.0486, 0.0000, 0.0486],\n        [0.4377, 0.0000, 0.2500, 0.4099, 0.0972, 0.0000, 0.1250, 0.0000, 0.0521],\n        [0.2430, 0.0000, 0.1701, 0.2430, 0.0729, 0.0000, 0.0729, 0.0000, 0.0243],\n        [0.1215, 0.0000, 0.0972, 0.1215, 0.0486, 0.0000, 0.0486, 0.0000, 0.0000],\n        [0.8194, 0.0000, 0.1806, 0.1877, 0.0000, 0.0000, 0.1493, 0.0000, 0.0521]],\n       grad_fn=<GSpMMBackward>)\n"
     ]
    }
   ],
   "source": [
    "g = dataset[0][0]\n",
    "\n",
    "g.ndata['feat'] = g.ndata['feat'].to(torch.float32)\n",
    "g.edata['feat'] = g.edata['feat'].to(torch.float32)\n",
    "\n",
    "g.edata['weight'] = F.softmax(nn.Linear(3, 1)(g.edata['feat']), dim=0)\n",
    "\n",
    "print(g.edata['weight'])\n",
    "\n",
    "print(g.ndata['feat'])\n",
    "\n",
    "g.update_all(\n",
    "    message_func=fn.u_mul_e('feat', 'weight', 'message'),\n",
    "    reduce_func=fn.sum('message', 'projection'),\n",
    ")\n",
    "\n",
    "print(g.ndata['projection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([40, 3])"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feats,\n",
    "        out_feats,\n",
    "        normalization: str = None,\n",
    "        activation: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._linear = nn.Linear(in_feats, out_feats)\n",
    "\n",
    "        if normalization is not None:\n",
    "            if normalization == 'batch':\n",
    "                self._normalization = nn.BatchNorm1d(out_feats)\n",
    "            elif normalization == 'layer':\n",
    "                self._normalization = nn.LayerNorm(out_feats)\n",
    "        else:\n",
    "            self._normalization = None\n",
    "\n",
    "        if activation is not None:\n",
    "            if activation == 'relu':\n",
    "                self._activation = nn.ReLU()\n",
    "            elif activation == 'leaky_relu':\n",
    "                self._activation = nn.LeakyReLU()\n",
    "            elif activation == 'sigmoid':\n",
    "                self._activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self._activation = None\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        x = self._linear(inputs)\n",
    "\n",
    "        if self._normalization is not None:\n",
    "            x = self._normalization(x)\n",
    "\n",
    "        if self._activation is not None:\n",
    "            x = self._activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MessageProjection(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        num_heads: int,\n",
    "        message_func: str,\n",
    "        reduce_func: str,\n",
    "        node_activation: str = None,\n",
    "        edge_activation: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._node_in_feats = node_in_feats\n",
    "        self._num_heads = num_heads\n",
    "        self._node_linear = LinearBlock(\n",
    "            node_in_feats,\n",
    "            node_in_feats * num_heads,\n",
    "            activation=node_activation,\n",
    "        )\n",
    "        self._edge_linear = LinearBlock(\n",
    "            edge_in_feats,\n",
    "            num_heads,\n",
    "            activation=edge_activation,\n",
    "        )\n",
    "\n",
    "        if message_func == 'add':\n",
    "            self._message_func = fn.u_add_e('projection', 'weight', 'message')\n",
    "        elif message_func == 'sub':\n",
    "            self._message_func = fn.u_sub_e('projection', 'weight', 'message')\n",
    "        elif message_func == 'mul':\n",
    "            self._message_func = fn.u_mul_e('projection', 'weight', 'message')\n",
    "        elif message_func == 'div':\n",
    "            self._message_func = fn.u_div_e('projection', 'weight', 'message')\n",
    "\n",
    "        if reduce_func == 'sum':\n",
    "            self._reduce_func = fn.sum('message', 'projection')\n",
    "        elif reduce_func == 'mean':\n",
    "            self._reduce_func = fn.mean('message', 'projection')\n",
    "\n",
    "    def forward(self, g: dgl.DGLGraph) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        g.ndata['projection'] = self._node_linear(g.ndata['feat'])\n",
    "        g.ndata['projection'] = g.ndata['projection'].view(\n",
    "            -1, self._num_heads, self._node_in_feats)\n",
    "\n",
    "        g.edata['weight'] = self._edge_linear(g.edata['feat'])\n",
    "        g.edata['weight'] = g.edata['weight'].view(-1, self._num_heads, 1)\n",
    "\n",
    "        g.update_all(\n",
    "            message_func=self._message_func,\n",
    "            reduce_func=self._reduce_func,\n",
    "        )\n",
    "\n",
    "        node_projection = g.ndata.pop('projection')\n",
    "        edge_projection = g.edata.pop('weight')\n",
    "\n",
    "        return node_projection, edge_projection\n",
    "\n",
    "\n",
    "class LinearProjection(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        num_heads: int,\n",
    "        activation: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._node_in_feats = node_in_feats\n",
    "        self._edge_in_feats = edge_in_feats\n",
    "        self._num_heads = num_heads\n",
    "        self._node_linear = LinearBlock(\n",
    "            node_in_feats,\n",
    "            node_in_feats * num_heads,\n",
    "            activation=activation,\n",
    "        )\n",
    "        self._edge_linear = LinearBlock(\n",
    "            edge_in_feats,\n",
    "            edge_in_feats * num_heads,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, g: dgl.DGLGraph) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        node_projection = self._node_linear(g.ndata['feat'])\n",
    "        node_projection = node_projection.view(\n",
    "            -1, self._num_heads, self._node_in_feats)\n",
    "\n",
    "        edge_projection = self._edge_linear(g.edata['feat'])\n",
    "        edge_projection = edge_projection.view(\n",
    "            -1, self._num_heads, self._edge_in_feats)\n",
    "\n",
    "        return node_projection, edge_projection\n",
    "\n",
    "\n",
    "class MultiMutualAttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        num_heads: int,\n",
    "        head_pooling_func: str,\n",
    "        message_func: str,\n",
    "        reduce_func: str,\n",
    "        message_projection_node_activation: str = None,\n",
    "        message_projection_edge_activation: str = None,\n",
    "        linear_projection_activation: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._node_scale_const = math.sqrt(node_in_feats)\n",
    "        self._edge_scale_const = math.sqrt(edge_in_feats)\n",
    "        self._head_pooling_func = head_pooling_func\n",
    "        self._query_linear = MessageProjection(\n",
    "            node_in_feats,\n",
    "            edge_in_feats,\n",
    "            num_heads,\n",
    "            message_func,\n",
    "            reduce_func,\n",
    "            message_projection_node_activation,\n",
    "            message_projection_edge_activation,\n",
    "        )\n",
    "        self._key_linear = MessageProjection(\n",
    "            node_in_feats,\n",
    "            edge_in_feats,\n",
    "            num_heads,\n",
    "            message_func,\n",
    "            reduce_func,\n",
    "            message_projection_node_activation,\n",
    "            message_projection_edge_activation,\n",
    "        )\n",
    "        self._value_linear = LinearProjection(\n",
    "            node_in_feats,\n",
    "            edge_in_feats,\n",
    "            num_heads,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "\n",
    "    def _calculate_attention_score(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        scale_const: float,\n",
    "    ):\n",
    "        attention_score = query @ torch.transpose(key, -1, -2)\n",
    "        attention_score = torch.exp(attention_score / scale_const).clamp(-5, 5)\n",
    "        attention_score = F.softmax(attention_score, dim=-1)\n",
    "\n",
    "        return attention_score\n",
    "\n",
    "    def forward(self, g: dgl.DGLGraph) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        node_query, edge_query = self._query_linear(g)\n",
    "        node_key, edge_key = self._key_linear(g)\n",
    "        node_value, edge_value = self._value_linear(g)\n",
    "\n",
    "        node_attention = self._calculate_attention_score(\n",
    "            node_query, node_key, self._node_scale_const)\n",
    "        edge_attention = self._calculate_attention_score(\n",
    "            edge_query, edge_key, self._edge_scale_const)\n",
    "\n",
    "        node_embedding = node_attention @ node_value\n",
    "        edge_embedding = edge_attention @ edge_value\n",
    "\n",
    "        if self._head_pooling_func == 'sum':\n",
    "            node_embedding = node_embedding.sum(-2)\n",
    "            edge_embedding = edge_embedding.sum(-2)\n",
    "        elif self._head_pooling_func == 'mean':\n",
    "            node_embedding = node_embedding.mean(-2)\n",
    "            edge_embedding = edge_embedding.mean(-2)\n",
    "\n",
    "        return node_embedding, edge_embedding\n",
    "\n",
    "\n",
    "\n",
    "node_emb, edge_emb = MultiMutualAttentionHead(9, 3, 4, 'sum', 'mul', 'sum', 'relu', 'sigmoid', 'relu')(g)\n",
    "\n",
    "edge_emb.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-8,  1],\n",
       "        [-9,  3]])"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "torch.tensor([[2, -1], [3, 0]]) @ torch.tensor([[-3, 1], [2, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "sqrt(): argument 'input' (position 1) must be Tensor, not int",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-b17d6d12958e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sqrt(): argument 'input' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "torch.sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feats,\n",
    "        out_feats,\n",
    "        normalization: str = None,\n",
    "        activation: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._linear = nn.Linear(in_feats, out_feats)\n",
    "\n",
    "        if normalization is not None:\n",
    "            if normalization == 'batch':\n",
    "                self._normalization = nn.BatchNorm1d(out_feats)\n",
    "            elif normalization == 'layer':\n",
    "                self._normalization = nn.LayerNorm(out_feats)\n",
    "        else:\n",
    "            self._normalization = None\n",
    "\n",
    "        if activation is not None:\n",
    "            if activation == 'relu':\n",
    "                self._activation = nn.ReLU()\n",
    "            elif activation == 'leaky_relu':\n",
    "                self._activation = nn.LeakyReLU()\n",
    "            elif activation == 'sigmoid':\n",
    "                self._activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self._activation = None\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        x = self._linear(inputs)\n",
    "\n",
    "        if self._normalization is not None:\n",
    "            x = self._normalization(x)\n",
    "\n",
    "        if self._activation is not None:\n",
    "            x = self._activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BilinearBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        out_feats: int,\n",
    "        activation: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._bilinear = nn.Bilinear(node_in_feats, edge_in_feats, out_feats)\n",
    "\n",
    "        if activation is not None:\n",
    "            if activation == 'relu':\n",
    "                self._activation = nn.ReLU()\n",
    "            elif activation == 'softplus':\n",
    "                self._activation == nn.Softplus()\n",
    "        else:\n",
    "            self._activation = None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        x = self._bilinear(node_inputs, edge_inputs)\n",
    "\n",
    "        if self._activation is not None:\n",
    "            x = self._activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MessageProjection(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        num_heads: int,\n",
    "        message_func: str,\n",
    "        reduce_func: str,\n",
    "        node_activation: str = None,\n",
    "        edge_activation: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._node_in_feats = node_in_feats\n",
    "        self._num_heads = num_heads\n",
    "        self._node_linear = LinearBlock(\n",
    "            node_in_feats,\n",
    "            node_in_feats * num_heads,\n",
    "            activation=node_activation,\n",
    "        )\n",
    "        self._edge_linear = LinearBlock(\n",
    "            edge_in_feats,\n",
    "            num_heads,\n",
    "            activation=edge_activation,\n",
    "        )\n",
    "\n",
    "        if message_func == 'add':\n",
    "            self._message_func = fn.u_add_e('projection', 'weight', 'message')\n",
    "        elif message_func == 'sub':\n",
    "            self._message_func = fn.u_sub_e('projection', 'weight', 'message')\n",
    "        elif message_func == 'mul':\n",
    "            self._message_func = fn.u_mul_e('projection', 'weight', 'message')\n",
    "        elif message_func == 'div':\n",
    "            self._message_func = fn.u_div_e('projection', 'weight', 'message')\n",
    "\n",
    "        if reduce_func == 'sum':\n",
    "            self._reduce_func = fn.sum('message', 'projection')\n",
    "        elif reduce_func == 'mean':\n",
    "            self._reduce_func = fn.mean('message', 'projection')\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        g.ndata['projection'] = self._node_linear(node_inputs)\n",
    "        g.ndata['projection'] = g.ndata['projection'].view(\n",
    "            -1, self._num_heads, self._node_in_feats)\n",
    "\n",
    "        g.edata['weight'] = self._edge_linear(edge_inputs)\n",
    "        g.edata['weight'] = g.edata['weight'].view(-1, self._num_heads, 1)\n",
    "\n",
    "        g.update_all(\n",
    "            message_func=self._message_func,\n",
    "            reduce_func=self._reduce_func,\n",
    "        )\n",
    "\n",
    "        node_projection = g.ndata.pop('projection')\n",
    "        edge_projection = g.edata.pop('weight')\n",
    "\n",
    "        return node_projection, edge_projection\n",
    "\n",
    "\n",
    "class LinearProjection(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        num_heads: int,\n",
    "        activation: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._node_in_feats = node_in_feats\n",
    "        self._edge_in_feats = edge_in_feats\n",
    "        self._num_heads = num_heads\n",
    "        self._node_linear = LinearBlock(\n",
    "            node_in_feats,\n",
    "            node_in_feats * num_heads,\n",
    "            activation=activation,\n",
    "        )\n",
    "        self._edge_linear = LinearBlock(\n",
    "            edge_in_feats,\n",
    "            edge_in_feats * num_heads,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        node_projection = self._node_linear(node_inputs)\n",
    "        node_projection = node_projection.view(\n",
    "            -1, self._num_heads, self._node_in_feats)\n",
    "\n",
    "        edge_projection = self._edge_linear(edge_inputs)\n",
    "        edge_projection = edge_projection.view(\n",
    "            -1, self._num_heads, self._edge_in_feats)\n",
    "\n",
    "        return node_projection, edge_projection\n",
    "\n",
    "\n",
    "class MutualMultiAttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        num_heads: int,\n",
    "        message_func: str,\n",
    "        reduce_func: str,\n",
    "        head_pooling_func: str,\n",
    "        weight_activation: str = None,\n",
    "        projection_activation: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._node_scale_const = math.sqrt(node_in_feats)\n",
    "        self._edge_scale_const = math.sqrt(edge_in_feats)\n",
    "        self._head_pooling_func = head_pooling_func\n",
    "        self._query_linear = MessageProjection(\n",
    "            node_in_feats,\n",
    "            edge_in_feats,\n",
    "            num_heads,\n",
    "            message_func,\n",
    "            reduce_func,\n",
    "            projection_activation,\n",
    "            weight_activation,\n",
    "        )\n",
    "        self._key_linear = MessageProjection(\n",
    "            node_in_feats,\n",
    "            edge_in_feats,\n",
    "            num_heads,\n",
    "            message_func,\n",
    "            reduce_func,\n",
    "            projection_activation,\n",
    "            weight_activation,\n",
    "        )\n",
    "        self._value_linear = LinearProjection(\n",
    "            node_in_feats,\n",
    "            edge_in_feats,\n",
    "            num_heads,\n",
    "            projection_activation,\n",
    "        )\n",
    "\n",
    "    def _calculate_attention_score(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        scale_const: float,\n",
    "    ):\n",
    "        attention_score = query @ torch.transpose(key, -1, -2)\n",
    "        attention_score = torch.exp(attention_score / scale_const).clamp(-5, 5)\n",
    "        attention_score = F.softmax(attention_score, dim=-1)\n",
    "\n",
    "        return attention_score\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        node_query, edge_query = self._query_linear(\n",
    "            g, node_inputs, edge_inputs)\n",
    "        node_key, edge_key = self._key_linear(g, node_inputs, edge_inputs)\n",
    "        node_value, edge_value = self._value_linear(\n",
    "            g, node_inputs, edge_inputs)\n",
    "\n",
    "        node_attention = self._calculate_attention_score(\n",
    "            node_query, node_key, self._node_scale_const)\n",
    "        edge_attention = self._calculate_attention_score(\n",
    "            edge_query, edge_key, self._edge_scale_const)\n",
    "\n",
    "        node_embedding = node_attention @ node_value\n",
    "        edge_embedding = edge_attention @ edge_value\n",
    "\n",
    "        if self._head_pooling_func == 'sum':\n",
    "            node_embedding = node_embedding.sum(-2)\n",
    "            edge_embedding = edge_embedding.sum(-2)\n",
    "        elif self._head_pooling_func == 'mean':\n",
    "            node_embedding = node_embedding.mean(-2)\n",
    "            edge_embedding = edge_embedding.mean(-2)\n",
    "\n",
    "        return node_embedding, edge_embedding\n",
    "\n",
    "\n",
    "class MutualAttentionTransformerLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        node_hidden_feats: int,\n",
    "        node_out_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        edge_hidden_feats: int,\n",
    "        edge_out_feats: int,\n",
    "        num_heads: int,\n",
    "        message_func: str,\n",
    "        reduce_func: str,\n",
    "        head_pooling_func: str,\n",
    "        residual: bool,\n",
    "        dropout: float,\n",
    "        embedding_normalization: str = None,\n",
    "        weight_activation: str = None,\n",
    "        projection_activation: str = None,\n",
    "        embedding_activation: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._residual = residual\n",
    "        self._mutual_multi_attention_head = MutualMultiAttentionHead(\n",
    "            node_in_feats,\n",
    "            edge_in_feats,\n",
    "            num_heads,\n",
    "            message_func,\n",
    "            reduce_func,\n",
    "            head_pooling_func,\n",
    "            weight_activation,\n",
    "            projection_activation,\n",
    "        )\n",
    "        self._node_embedding_dropout_1 = nn.Dropout(dropout)\n",
    "        self._node_embedding_dropout_2 = nn.Dropout(dropout)\n",
    "        self._edge_embedding_dropout_1 = nn.Dropout(dropout)\n",
    "        self._edge_embedding_dropout_2 = nn.Dropout(dropout)\n",
    "        self._node_embedding_linear_1 = LinearBlock(\n",
    "            node_in_feats,\n",
    "            node_hidden_feats,\n",
    "            normalization=embedding_normalization,\n",
    "            activation=embedding_activation,\n",
    "        )\n",
    "        self._node_embedding_linear_2 = LinearBlock(\n",
    "            node_hidden_feats,\n",
    "            node_out_feats,\n",
    "            normalization=embedding_normalization,\n",
    "            activation=embedding_activation,\n",
    "        )\n",
    "        self._edge_embedding_linear_1 = LinearBlock(\n",
    "            edge_in_feats,\n",
    "            edge_hidden_feats,\n",
    "            normalization=embedding_normalization,\n",
    "            activation=embedding_activation,\n",
    "        )\n",
    "        self._edge_embedding_linear_2 = LinearBlock(\n",
    "            edge_hidden_feats,\n",
    "            edge_out_feats,\n",
    "            normalization=embedding_normalization,\n",
    "            activation=embedding_activation,\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        node_embedding, edge_embedding = self._mutual_multi_attention_head(\n",
    "            g, node_inputs, edge_inputs)\n",
    "\n",
    "        node_embedding = self._node_embedding_dropout_1(node_embedding)\n",
    "        edge_embedding = self._edge_embedding_dropout_1(edge_embedding)\n",
    "\n",
    "        if self._residual:\n",
    "            node_embedding += node_inputs\n",
    "            edge_embedding += edge_inputs\n",
    "\n",
    "        node_embedding = self._node_embedding_linear_1(node_embedding)\n",
    "        edge_embedding = self._edge_embedding_linear_1(edge_embedding)\n",
    "\n",
    "        node_embedding = self._node_embedding_dropout_2(node_embedding)\n",
    "        edge_embedding = self._edge_embedding_dropout_2(edge_embedding)\n",
    "\n",
    "        node_embedding = self._node_embedding_linear_2(node_embedding)\n",
    "        edge_embedding = self._edge_embedding_linear_2(edge_embedding)\n",
    "\n",
    "        return node_embedding, edge_embedding\n",
    "\n",
    "\n",
    "class GraphMututalAttentionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        node_hidden_feats: int,\n",
    "        node_out_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        edge_hidden_feats: int,\n",
    "        edge_out_feats: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        message_func: str,\n",
    "        reduce_func: str,\n",
    "        head_pooling_func: str,\n",
    "        readout_pooling_func: str,\n",
    "        residual: bool,\n",
    "        dropout: float,\n",
    "        embedding_normalization: str = None,\n",
    "        weight_activation: str = None,\n",
    "        projection_activation: str = None,\n",
    "        embedding_activation: str = None,\n",
    "        readout_activation: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._node_out_feats = node_out_feats\n",
    "        self._edge_out_feats = edge_out_feats\n",
    "        self._transformer_layers = self._create_transformer_layers(\n",
    "            node_in_feats,\n",
    "            node_hidden_feats,\n",
    "            node_out_feats,\n",
    "            edge_in_feats,\n",
    "            edge_hidden_feats,\n",
    "            edge_out_feats,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            message_func,\n",
    "            reduce_func,\n",
    "            head_pooling_func,\n",
    "            residual,\n",
    "            dropout,\n",
    "            embedding_normalization,\n",
    "            weight_activation,\n",
    "            projection_activation,\n",
    "            embedding_activation,\n",
    "        )\n",
    "\n",
    "        if readout_pooling_func == 'sum':\n",
    "            self._readout_pooling = dgl.nn.pytorch.SumPooling()\n",
    "        elif readout_pooling_func == 'mean':\n",
    "            self._readout_pooling = dgl.nn.pytorch.AvgPooling()\n",
    "\n",
    "        self._bilinear = BilinearBlock(\n",
    "            node_out_feats,\n",
    "            edge_out_feats,\n",
    "            1,\n",
    "            readout_activation,\n",
    "        )\n",
    "\n",
    "    def _create_transformer_layers(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        node_hidden_feats: int,\n",
    "        node_out_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        edge_hidden_feats: int,\n",
    "        edge_out_feats: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        message_func: str,\n",
    "        reduce_func: str,\n",
    "        head_pooling_func: str,\n",
    "        residual: bool,\n",
    "        dropout: float,\n",
    "        embedding_normalization: str = None,\n",
    "        weight_activation: str = None,\n",
    "        projection_activation: str = None,\n",
    "        embedding_activation: str = None,\n",
    "    ) -> nn.ModuleList:\n",
    "        transformer_layers = nn.ModuleList()\n",
    "\n",
    "        if num_layers > 1:\n",
    "            transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                node_in_feats,\n",
    "                node_in_feats * node_hidden_feats,\n",
    "                node_hidden_feats,\n",
    "                edge_in_feats,\n",
    "                edge_in_feats * edge_hidden_feats,\n",
    "                edge_hidden_feats,\n",
    "                num_heads,\n",
    "                message_func,\n",
    "                reduce_func,\n",
    "                head_pooling_func,\n",
    "                residual,\n",
    "                dropout,\n",
    "                embedding_normalization,\n",
    "                weight_activation,\n",
    "                projection_activation,\n",
    "                embedding_activation,\n",
    "            ))\n",
    "\n",
    "            for _ in range(num_layers - 2):\n",
    "                transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                    node_hidden_feats,\n",
    "                    node_hidden_feats * node_hidden_feats,\n",
    "                    node_hidden_feats,\n",
    "                    edge_hidden_feats,\n",
    "                    edge_hidden_feats * edge_hidden_feats,\n",
    "                    edge_hidden_feats,\n",
    "                    num_heads,\n",
    "                    message_func,\n",
    "                    reduce_func,\n",
    "                    head_pooling_func,\n",
    "                    residual,\n",
    "                    dropout,\n",
    "                    embedding_normalization,\n",
    "                    weight_activation,\n",
    "                    projection_activation,\n",
    "                    embedding_activation,\n",
    "                ))\n",
    "\n",
    "            transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                node_hidden_feats,\n",
    "                node_hidden_feats * node_out_feats,\n",
    "                node_out_feats,\n",
    "                edge_hidden_feats,\n",
    "                edge_hidden_feats * edge_out_feats,\n",
    "                edge_out_feats,\n",
    "                num_heads,\n",
    "                message_func,\n",
    "                reduce_func,\n",
    "                head_pooling_func,\n",
    "                residual,\n",
    "                dropout,\n",
    "                embedding_normalization,\n",
    "                weight_activation,\n",
    "                projection_activation,\n",
    "                embedding_activation,\n",
    "            ))\n",
    "\n",
    "        return transformer_layers\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        node_embedding = node_inputs\n",
    "        edge_embedding = edge_inputs\n",
    "\n",
    "        for transformer_layer in self._transformer_layers:\n",
    "            node_embedding, edge_embedding = transformer_layer(\n",
    "                g, node_embedding, edge_embedding)\n",
    "\n",
    "        node_embedding = self._readout_pooling(g, node_embedding)\n",
    "        edge_embedding = self._readout_pooling(g.line_graph(backtracking=False), edge_embedding)\n",
    "\n",
    "        readout = self._bilinear(node_embedding, edge_embedding)\n",
    "\n",
    "        return readout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphMututalAttentionTransformer(\n",
    "    node_in_feats=9,\n",
    "    node_hidden_feats=9,\n",
    "    node_out_feats=9,\n",
    "    edge_in_feats=3,\n",
    "    edge_hidden_feats=3,\n",
    "    edge_out_feats=3,\n",
    "    num_layers=5,\n",
    "    num_heads=5,\n",
    "    message_func='mul',\n",
    "    reduce_func='sum',\n",
    "    head_pooling_func='sum',\n",
    "    readout_pooling_func='mean',\n",
    "    residual=True,\n",
    "    dropout=0.01,\n",
    "    embedding_normalization='batch',\n",
    "    weight_activation='sigmoid',\n",
    "    projection_activation='leaky_relu',\n",
    "    embedding_activation='leaky_relu',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2178]], grad_fn=<AddBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "model(g, g.ndata['feat'], g.edata['feat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.4709774041548371\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer\n",
    "\n",
    "time = 0\n",
    "\n",
    "for _ in range(100_000):\n",
    "    start = default_timer()\n",
    "\n",
    "    g.line_graph(backtracking=False)\n",
    "\n",
    "    stop = default_timer()\n",
    "\n",
    "    time += stop - start\n",
    "\n",
    "print(time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(-8.0466e-07)"
      ]
     },
     "metadata": {},
     "execution_count": 77
    }
   ],
   "source": [
    "torch.det(torch.tensor([[-1, 4, -1], [2, 1, 2], [3, -2, 3]], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}