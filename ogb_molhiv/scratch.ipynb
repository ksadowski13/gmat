{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd04884a46182a36c52786c58ae747f94ef8bde10a8978cfcd06107010b24cc677d",
   "display_name": "Python 3.8.8 64-bit ('ogb-lsc-env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ogb.graphproppred import DglGraphPropPredDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DglGraphPropPredDataset(root='/home/ksadowski/datasets', name='ogbg-molhiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0172],\n        [0.0172],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0172],\n        [0.0172],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0172],\n        [0.0172],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0172],\n        [0.0172],\n        [0.0270],\n        [0.0270],\n        [0.0270],\n        [0.0270]], grad_fn=<SoftmaxBackward>)\ntensor([[-0.1864,  0.0000, -0.0671, -0.0890, -0.0097,  0.0000, -0.0383,  0.0000,\n         -0.0168],\n        [-0.4864,  0.0000, -0.2168, -0.4386, -0.0606,  0.0000, -0.1084,  0.0000,\n         -0.0618],\n        [-1.1657,  0.0000, -0.3910, -0.5069, -0.0477,  0.0000, -0.2338,  0.0000,\n         -0.1007],\n        [-0.9039,  0.0000, -0.3859, -0.8102, -0.0993,  0.0000, -0.1929,  0.0000,\n         -0.1171],\n        [-2.8109,  0.0000, -0.9020, -1.1519, -0.0966,  0.0000, -0.5561,  0.0000,\n         -0.2364],\n        [-0.9039,  0.0000, -0.3859, -0.8102, -0.0993,  0.0000, -0.1929,  0.0000,\n         -0.1171],\n        [-1.1657,  0.0000, -0.3910, -0.5069, -0.0477,  0.0000, -0.2338,  0.0000,\n         -0.1007],\n        [-0.4864,  0.0000, -0.2168, -0.4386, -0.0606,  0.0000, -0.1084,  0.0000,\n         -0.0618],\n        [-0.1864,  0.0000, -0.0671, -0.0890, -0.0097,  0.0000, -0.0383,  0.0000,\n         -0.0168],\n        [-0.7750,  0.0000, -0.3427, -0.6980, -0.0944,  0.0000, -0.1714,  0.0000,\n         -0.0988],\n        [-0.9039,  0.0000, -0.3859, -0.8102, -0.0993,  0.0000, -0.1929,  0.0000,\n         -0.1171],\n        [-1.1657,  0.0000, -0.3910, -0.5069, -0.0477,  0.0000, -0.2338,  0.0000,\n         -0.1007],\n        [-0.4864,  0.0000, -0.2168, -0.4386, -0.0606,  0.0000, -0.1084,  0.0000,\n         -0.0618],\n        [-0.1864,  0.0000, -0.0671, -0.0890, -0.0097,  0.0000, -0.0383,  0.0000,\n         -0.0168],\n        [-0.7750,  0.0000, -0.3427, -0.6980, -0.0944,  0.0000, -0.1714,  0.0000,\n         -0.0988],\n        [-1.1657,  0.0000, -0.3910, -0.5069, -0.0477,  0.0000, -0.2338,  0.0000,\n         -0.1007],\n        [-0.4864,  0.0000, -0.2168, -0.4386, -0.0606,  0.0000, -0.1084,  0.0000,\n         -0.0618],\n        [-0.1864,  0.0000, -0.0671, -0.0890, -0.0097,  0.0000, -0.0383,  0.0000,\n         -0.0168],\n        [-0.9039,  0.0000, -0.3859, -0.8102, -0.0993,  0.0000, -0.1929,  0.0000,\n         -0.1171]], grad_fn=<GSpMMBackward>)\ntensor([[-0.0131,  0.0000, -0.0058, -0.0118, -0.0016,  0.0000, -0.0029,  0.0000,\n         -0.0017],\n        [-0.0364,  0.0000, -0.0123, -0.0161, -0.0015,  0.0000, -0.0073,  0.0000,\n         -0.0032],\n        [-0.0495,  0.0000, -0.0217, -0.0446, -0.0059,  0.0000, -0.0109,  0.0000,\n         -0.0063],\n        [-0.0958,  0.0000, -0.0310, -0.0398, -0.0034,  0.0000, -0.0190,  0.0000,\n         -0.0081],\n        [-0.0975,  0.0000, -0.0416, -0.0874, -0.0107,  0.0000, -0.0208,  0.0000,\n         -0.0126],\n        [-0.0958,  0.0000, -0.0310, -0.0398, -0.0034,  0.0000, -0.0190,  0.0000,\n         -0.0081],\n        [-0.0495,  0.0000, -0.0217, -0.0446, -0.0059,  0.0000, -0.0109,  0.0000,\n         -0.0063],\n        [-0.0364,  0.0000, -0.0123, -0.0161, -0.0015,  0.0000, -0.0073,  0.0000,\n         -0.0032],\n        [-0.0131,  0.0000, -0.0058, -0.0118, -0.0016,  0.0000, -0.0029,  0.0000,\n         -0.0017],\n        [-0.0628,  0.0000, -0.0211, -0.0273, -0.0026,  0.0000, -0.0126,  0.0000,\n         -0.0054],\n        [-0.0958,  0.0000, -0.0310, -0.0398, -0.0034,  0.0000, -0.0190,  0.0000,\n         -0.0081],\n        [-0.0495,  0.0000, -0.0217, -0.0446, -0.0059,  0.0000, -0.0109,  0.0000,\n         -0.0063],\n        [-0.0364,  0.0000, -0.0123, -0.0161, -0.0015,  0.0000, -0.0073,  0.0000,\n         -0.0032],\n        [-0.0131,  0.0000, -0.0058, -0.0118, -0.0016,  0.0000, -0.0029,  0.0000,\n         -0.0017],\n        [-0.0628,  0.0000, -0.0211, -0.0273, -0.0026,  0.0000, -0.0126,  0.0000,\n         -0.0054],\n        [-0.0495,  0.0000, -0.0217, -0.0446, -0.0059,  0.0000, -0.0109,  0.0000,\n         -0.0063],\n        [-0.0364,  0.0000, -0.0123, -0.0161, -0.0015,  0.0000, -0.0073,  0.0000,\n         -0.0032],\n        [-0.0131,  0.0000, -0.0058, -0.0118, -0.0016,  0.0000, -0.0029,  0.0000,\n         -0.0017],\n        [-0.0958,  0.0000, -0.0310, -0.0398, -0.0034,  0.0000, -0.0190,  0.0000,\n         -0.0081]], grad_fn=<GSpMMBackward>)\n"
     ]
    }
   ],
   "source": [
    "g = dataset[0][0]\n",
    "\n",
    "g.ndata['feat'] = g.ndata['feat'].to(torch.float32)\n",
    "g.edata['feat'] = g.edata['feat'].to(torch.float32)\n",
    "\n",
    "g.edata['weight'] = F.softmax(nn.Linear(3, 1)(g.edata['feat']), dim=0)\n",
    "\n",
    "print(g.edata['weight'])\n",
    "\n",
    "print(g.ndata['feat'])\n",
    "\n",
    "g.update_all(\n",
    "    message_func=fn.u_mul_e('feat', 'weight', 'message'),\n",
    "    reduce_func=fn.sum('message', 'projection'),\n",
    ")\n",
    "\n",
    "print(g.ndata['projection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<dgl.function.message.BinaryMessageFunction at 0x7f2f0ac0ecd0>"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "class LinearBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feats,\n",
    "        out_feats,\n",
    "        normalization: str = None,\n",
    "        activation: str = None,\n",
    "        dim: int = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._linear = nn.Linear(in_feats, out_feats)\n",
    "\n",
    "        if normalization is not None:\n",
    "            if normalization == 'batch':\n",
    "                self._normalization = nn.BatchNorm1d(out_feats)\n",
    "            elif normalization == 'layer':\n",
    "                self._normalization = nn.LayerNorm(out_feats)\n",
    "        else:\n",
    "            self._normalization = None\n",
    "\n",
    "        if activation is not None:\n",
    "            if activation == 'relu':\n",
    "                self._activation = nn.ReLU()\n",
    "            elif activation == 'leaky_relu':\n",
    "                self._activation = nn.LeakyReLU()\n",
    "            elif activation == 'sigmoid':\n",
    "                self._activation = nn.Sigmoid()\n",
    "            elif activation == 'softmax' and dim is not None:\n",
    "                self._activation = nn.Softmax(dim=dim)\n",
    "        else:\n",
    "            self._activation = None\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        x = self._linear(inputs)\n",
    "\n",
    "        if self._normalization is not None:\n",
    "            x = self._normalization(x)\n",
    "\n",
    "        if self._activation is not None:\n",
    "            x = self._activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MessageProjection(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        num_heads: int,\n",
    "        message_weight_func: str,\n",
    "        reduce_func: str,\n",
    "        weight_activation: str = None,\n",
    "        message_activation: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._node_in_feats = node_in_feats\n",
    "        self._edge_in_feats = edge_in_feats\n",
    "        self._num_heads = num_heads\n",
    "\n",
    "        if message_weight_func == 'add':\n",
    "            self._message_weight_func = fn.u_add_e('feat', 'weight', 'message')\n",
    "        elif message_weight_func == 'sub':\n",
    "            self._message_weight_func = fn.u_sub_e('feat', 'weight', 'message')\n",
    "        elif message_weight_func == 'mul':\n",
    "            self._message_weight_func = fn.u_mul_e('feat', 'weight', 'message')\n",
    "        elif message_weight_func == 'div':\n",
    "            self._message_weight_func = fn.u_div_e('feat', 'weight', 'message')\n",
    "\n",
    "        if reduce_func == 'sum':\n",
    "            self._reduce_func = fn.sum('message', 'projection')\n",
    "        elif reduce_func == 'mean':\n",
    "            self._reduce_func = fn.mean('message', 'projection')\n",
    "\n",
    "        self._edge_linear = LinearBlock(\n",
    "            edge_in_feats, \n",
    "            1, \n",
    "            activation=weight_activation,\n",
    "            dim=0 if weight_activation == 'softmax' else None,\n",
    "        )\n",
    "        self._message_linear = LinearBlock(\n",
    "            node_in_feats, \n",
    "            node_in_feats * num_heads,\n",
    "            activation=message_activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, g: dgl.DGLGraph) -> torch.Tensor:\n",
    "        g.edata['weight'] = self._edge_linear(g.edata['feat'])\n",
    "\n",
    "        g.update_all(\n",
    "            message_func=self._message_weight_func,\n",
    "            reduce_func=self._reduce_func,\n",
    "        )\n",
    "\n",
    "        message_projection = self._message_linear(g.ndata.pop('projection'))\n",
    "        message_projection = message_projection.view(\n",
    "            -1, self._num_heads, self._node_in_feats)\n",
    "\n",
    "        edge_projection = g.edata.pop('weight')\n",
    "\n",
    "        return message_projection, edge_projection\n",
    "\n",
    "\n",
    "\n",
    "Q_message_projection = MessageProjection(9, 3, 4, 'mul', 'mean', 'sigmoid', 'relu')\n",
    "K_message_projection = MessageProjection(9, 3, 4, 'mul', 'mean', 'sigmoid', 'relu')\n",
    "\n",
    "g.ndata['query'], g.edata['query'] = Q_message_projection(g)\n",
    "g.ndata['key'], g.edata['key'] = K_message_projection(g)\n",
    "\n",
    "g.edata['query'] @ g.edata['key'].T @ g.edata['feat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}