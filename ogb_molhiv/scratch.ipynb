{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd04884a46182a36c52786c58ae747f94ef8bde10a8978cfcd06107010b24cc677d",
   "display_name": "Python 3.8.8 64-bit ('ogb-lsc-env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ogb.graphproppred import DglGraphPropPredDataset\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DglGraphPropPredDataset(root='/home/ksadowski/datasets', name='ogbg-molhiv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0278],\n        [0.0278],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0278],\n        [0.0278],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0278],\n        [0.0278],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0278],\n        [0.0278],\n        [0.0243],\n        [0.0243],\n        [0.0243],\n        [0.0243]], grad_fn=<SoftmaxBackward>)\ntensor([[ 5.,  0.,  4.,  5.,  3.,  0.,  2.,  0.,  0.],\n        [ 5.,  0.,  4.,  5.,  2.,  0.,  2.,  0.,  0.],\n        [ 5.,  0.,  3.,  5.,  0.,  0.,  1.,  0.,  1.],\n        [ 7.,  0.,  2.,  6.,  0.,  0.,  1.,  0.,  1.],\n        [28.,  0.,  4.,  2.,  0.,  0.,  5.,  0.,  1.],\n        [ 7.,  0.,  2.,  6.,  0.,  0.,  1.,  0.,  1.],\n        [ 5.,  0.,  3.,  5.,  0.,  0.,  1.,  0.,  1.],\n        [ 5.,  0.,  4.,  5.,  2.,  0.,  2.,  0.,  0.],\n        [ 5.,  0.,  4.,  5.,  3.,  0.,  2.,  0.,  0.],\n        [ 5.,  0.,  4.,  5.,  2.,  0.,  2.,  0.,  1.],\n        [ 7.,  0.,  2.,  6.,  0.,  0.,  1.,  0.,  1.],\n        [ 5.,  0.,  3.,  5.,  0.,  0.,  1.,  0.,  1.],\n        [ 5.,  0.,  4.,  5.,  2.,  0.,  2.,  0.,  0.],\n        [ 5.,  0.,  4.,  5.,  3.,  0.,  2.,  0.,  0.],\n        [ 5.,  0.,  4.,  5.,  2.,  0.,  2.,  0.,  1.],\n        [ 5.,  0.,  3.,  5.,  0.,  0.,  1.,  0.,  1.],\n        [ 5.,  0.,  4.,  5.,  2.,  0.,  2.,  0.,  0.],\n        [ 5.,  0.,  4.,  5.,  3.,  0.,  2.,  0.,  0.],\n        [ 7.,  0.,  2.,  6.,  0.,  0.,  1.,  0.,  1.]])\ntensor([[0.1215, 0.0000, 0.0972, 0.1215, 0.0486, 0.0000, 0.0486, 0.0000, 0.0000],\n        [0.2430, 0.0000, 0.1701, 0.2430, 0.0729, 0.0000, 0.0729, 0.0000, 0.0243],\n        [0.4377, 0.0000, 0.2500, 0.4099, 0.0972, 0.0000, 0.1250, 0.0000, 0.0521],\n        [0.8194, 0.0000, 0.1806, 0.1877, 0.0000, 0.0000, 0.1493, 0.0000, 0.0521],\n        [0.6803, 0.0000, 0.1944, 0.5831, 0.0000, 0.0000, 0.0972, 0.0000, 0.0972],\n        [0.8194, 0.0000, 0.1806, 0.1877, 0.0000, 0.0000, 0.1493, 0.0000, 0.0521],\n        [0.4377, 0.0000, 0.2500, 0.4099, 0.0972, 0.0000, 0.1250, 0.0000, 0.0521],\n        [0.2430, 0.0000, 0.1701, 0.2430, 0.0729, 0.0000, 0.0729, 0.0000, 0.0243],\n        [0.1215, 0.0000, 0.0972, 0.1215, 0.0486, 0.0000, 0.0486, 0.0000, 0.0000],\n        [0.2430, 0.0000, 0.1458, 0.2430, 0.0000, 0.0000, 0.0486, 0.0000, 0.0486],\n        [0.8194, 0.0000, 0.1806, 0.1877, 0.0000, 0.0000, 0.1493, 0.0000, 0.0521],\n        [0.4377, 0.0000, 0.2500, 0.4099, 0.0972, 0.0000, 0.1250, 0.0000, 0.0521],\n        [0.2430, 0.0000, 0.1701, 0.2430, 0.0729, 0.0000, 0.0729, 0.0000, 0.0243],\n        [0.1215, 0.0000, 0.0972, 0.1215, 0.0486, 0.0000, 0.0486, 0.0000, 0.0000],\n        [0.2430, 0.0000, 0.1458, 0.2430, 0.0000, 0.0000, 0.0486, 0.0000, 0.0486],\n        [0.4377, 0.0000, 0.2500, 0.4099, 0.0972, 0.0000, 0.1250, 0.0000, 0.0521],\n        [0.2430, 0.0000, 0.1701, 0.2430, 0.0729, 0.0000, 0.0729, 0.0000, 0.0243],\n        [0.1215, 0.0000, 0.0972, 0.1215, 0.0486, 0.0000, 0.0486, 0.0000, 0.0000],\n        [0.8194, 0.0000, 0.1806, 0.1877, 0.0000, 0.0000, 0.1493, 0.0000, 0.0521]],\n       grad_fn=<GSpMMBackward>)\n"
     ]
    }
   ],
   "source": [
    "g = dataset[0][0]\n",
    "\n",
    "g.ndata['feat'] = g.ndata['feat'].to(torch.float32)\n",
    "g.edata['feat'] = g.edata['feat'].to(torch.float32)\n",
    "\n",
    "g.edata['weight'] = F.softmax(nn.Linear(3, 1)(g.edata['feat']), dim=0)\n",
    "\n",
    "print(g.edata['weight'])\n",
    "\n",
    "print(g.ndata['feat'])\n",
    "\n",
    "g.update_all(\n",
    "    message_func=fn.u_mul_e('feat', 'weight', 'message'),\n",
    "    reduce_func=fn.sum('message', 'projection'),\n",
    ")\n",
    "\n",
    "print(g.ndata['projection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([19, 9])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LinearBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feats,\n",
    "        out_feats,\n",
    "        normalization: str = None,\n",
    "        activation: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._linear = nn.Linear(in_feats, out_feats)\n",
    "\n",
    "        if normalization is not None:\n",
    "            if normalization == 'batch':\n",
    "                self._normalization = nn.BatchNorm1d(out_feats)\n",
    "            elif normalization == 'layer':\n",
    "                self._normalization = nn.LayerNorm(out_feats)\n",
    "        else:\n",
    "            self._normalization = None\n",
    "\n",
    "        if activation is not None:\n",
    "            if activation == 'relu':\n",
    "                self._activation = nn.ReLU()\n",
    "            elif activation == 'leaky_relu':\n",
    "                self._activation = nn.LeakyReLU()\n",
    "            elif activation == 'sigmoid':\n",
    "                self._activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self._activation = None\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        x = self._linear(inputs)\n",
    "\n",
    "        if self._normalization is not None:\n",
    "            x = self._normalization(x)\n",
    "\n",
    "        if self._activation is not None:\n",
    "            x = self._activation(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MessageProjection(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        num_heads: int,\n",
    "        message_func: str,\n",
    "        reduce_func: str,\n",
    "        node_activation: str = None,\n",
    "        edge_activation: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._node_in_feats = node_in_feats\n",
    "        self._num_heads = num_heads\n",
    "        self._node_linear = LinearBlock(\n",
    "            node_in_feats,\n",
    "            node_in_feats * num_heads,\n",
    "            activation=node_activation,\n",
    "        )\n",
    "        self._edge_linear = LinearBlock(\n",
    "            edge_in_feats,\n",
    "            num_heads,\n",
    "            activation=edge_activation,\n",
    "        )\n",
    "\n",
    "        if message_func == 'add':\n",
    "            self._message_func = fn.u_add_e('projection', 'weight', 'message')\n",
    "        elif message_func == 'sub':\n",
    "            self._message_func = fn.u_sub_e('projection', 'weight', 'message')\n",
    "        elif message_func == 'mul':\n",
    "            self._message_func = fn.u_mul_e('projection', 'weight', 'message')\n",
    "        elif message_func == 'div':\n",
    "            self._message_func = fn.u_div_e('projection', 'weight', 'message')\n",
    "\n",
    "        if reduce_func == 'sum':\n",
    "            self._reduce_func = fn.sum('message', 'projection')\n",
    "        elif reduce_func == 'mean':\n",
    "            self._reduce_func = fn.mean('message', 'projection')\n",
    "\n",
    "    def forward(self, g: dgl.DGLGraph) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        g.ndata['projection'] = self._node_linear(g.ndata['feat'])\n",
    "        g.ndata['projection'] = g.ndata['projection'].view(\n",
    "            -1, self._num_heads, self._node_in_feats)\n",
    "\n",
    "        g.edata['weight'] = self._edge_linear(g.edata['feat'])\n",
    "        g.edata['weight'] = g.edata['weight'].view(-1, self._num_heads, 1)\n",
    "\n",
    "        g.update_all(\n",
    "            message_func=self._message_func,\n",
    "            reduce_func=self._reduce_func,\n",
    "        )\n",
    "\n",
    "        node_projection = g.ndata.pop('projection')\n",
    "        edge_projection = g.edata.pop('weight')\n",
    "\n",
    "        return node_projection, edge_projection\n",
    "\n",
    "\n",
    "class LinearProjection(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        num_heads: int,\n",
    "        activation: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._node_in_feats = node_in_feats\n",
    "        self._edge_in_feats = edge_in_feats\n",
    "        self._num_heads = num_heads\n",
    "        self._node_linear = LinearBlock(\n",
    "            node_in_feats,\n",
    "            node_in_feats * num_heads,\n",
    "            activation=activation,\n",
    "        )\n",
    "        self._edge_linear = LinearBlock(\n",
    "            edge_in_feats,\n",
    "            edge_in_feats * num_heads,\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "    def forward(self, g: dgl.DGLGraph) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        node_projection = self._node_linear(g.ndata['feat'])\n",
    "        node_projection = node_projection.view(\n",
    "            -1, self._num_heads, self._node_in_feats)\n",
    "\n",
    "        edge_projection = self._edge_linear(g.edata['feat'])\n",
    "        edge_projection = edge_projection.view(\n",
    "            -1, self._num_heads, self._edge_in_feats)\n",
    "\n",
    "        return node_projection, edge_projection\n",
    "\n",
    "\n",
    "class MultiMutualAttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        num_heads: int,\n",
    "        head_pooling_func: str,\n",
    "        message_func: str,\n",
    "        reduce_func: str,\n",
    "        message_projection_node_activation: str = None,\n",
    "        message_projection_edge_activation: str = None,\n",
    "        linear_projection_activation: str = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self._node_scale_const = math.sqrt(node_in_feats)\n",
    "        self._edge_scale_const = math.sqrt(edge_in_feats)\n",
    "        self._head_pooling_func = head_pooling_func\n",
    "        self._query_linear = MessageProjection(\n",
    "            node_in_feats,\n",
    "            edge_in_feats,\n",
    "            num_heads,\n",
    "            message_func,\n",
    "            reduce_func,\n",
    "            message_projection_node_activation,\n",
    "            message_projection_edge_activation,\n",
    "        )\n",
    "        self._key_linear = MessageProjection(\n",
    "            node_in_feats,\n",
    "            edge_in_feats,\n",
    "            num_heads,\n",
    "            message_func,\n",
    "            reduce_func,\n",
    "            message_projection_node_activation,\n",
    "            message_projection_edge_activation,\n",
    "        )\n",
    "        self._value_linear = LinearProjection(\n",
    "            node_in_feats,\n",
    "            edge_in_feats,\n",
    "            num_heads,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "\n",
    "    def _calculate_attention_score(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        scale_const: float,\n",
    "    ):\n",
    "        attention_score = query @ torch.transpose(key, -1, -2)\n",
    "        attention_score = torch.exp(attention_score / scale_const).clamp(-5, 5)\n",
    "        attention_score = F.softmax(attention_score, dim=-1)\n",
    "\n",
    "        return attention_score\n",
    "\n",
    "    def forward(self, g: dgl.DGLGraph) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        node_query, edge_query = self._query_linear(g)\n",
    "        node_key, edge_key = self._key_linear(g)\n",
    "        node_value, edge_value = self._value_linear(g)\n",
    "\n",
    "        node_attention = self._calculate_attention_score(\n",
    "            node_query, node_key, self._node_scale_const)\n",
    "        edge_attention = self._calculate_attention_score(\n",
    "            edge_query, edge_key, self._edge_scale_const)\n",
    "\n",
    "        node_embedding = node_attention @ node_value\n",
    "        edge_embedding = edge_attention @ edge_value\n",
    "\n",
    "        if self._head_pooling_func == 'sum':\n",
    "            node_embedding = node_embedding.sum(-2)\n",
    "            edge_embedding = edge_embedding.sum(-2)\n",
    "        elif self._head_pooling_func == 'mean':\n",
    "            node_embedding = node_embedding.mean(-2)\n",
    "            edge_embedding = edge_embedding.mean(-2)\n",
    "\n",
    "        return node_embedding, edge_embedding\n",
    "\n",
    "\n",
    "\n",
    "node_emb, edge_emb = MultiMutualAttentionHead(9, 3, 4, 'sum', 'mul', 'sum', 'relu', 'sigmoid', 'relu')(g)\n",
    "\n",
    "edge_emb.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}