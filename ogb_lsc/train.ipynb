{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd04884a46182a36c52786c58ae747f94ef8bde10a8978cfcd06107010b24cc677d",
   "display_name": "Python 3.8.8 64-bit ('ogb-lsc-env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Dependencies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from timeit import default_timer\n",
    "from typing import Union, Tuple\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from ogb.lsc import DglPCQM4MDataset\n",
    "from ogb.utils import smiles2graph\n",
    "from tqdm import trange\n",
    "\n",
    "torch.manual_seed(13)"
   ]
  },
  {
   "source": [
    "# Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessedPCQM4M(dgl.data.DGLDataset):\n",
    "    def __init__(self, molecules_path: str, molecules_lg_path: str):\n",
    "        self.molecules_path = molecules_path\n",
    "        self.molecules_lg_path = molecules_lg_path\n",
    "        self.graphs = []\n",
    "        self.line_graphs = []\n",
    "        self.labels = []\n",
    "        super().__init__(name='processed_PCQM4M')\n",
    "\n",
    "    def process(self):  \n",
    "        start = default_timer()\n",
    "\n",
    "        self.graphs, labels = dgl.data.utils.load_graphs(self.molecules_path)\n",
    "        self.line_graphs, _ = dgl.data.utils.load_graphs(self.molecules_lg_path)\n",
    "\n",
    "        # self.labels = [label for label in labels.values()]\n",
    "\n",
    "        for i in range(len(self.graphs)):\n",
    "            self.labels.append(labels[str(i)])\n",
    "\n",
    "        stop = default_timer()\n",
    "\n",
    "        print(f'Processed data loading time: {(stop - start) / 60} min.')\n",
    "\n",
    "    def __getitem__(self, idx: Union[int, torch.Tensor]):\n",
    "        if isinstance(idx, int):\n",
    "            return self.graphs[idx], self.line_graphs[idx], self.labels[idx]\n",
    "        elif torch.is_tensor(idx) and idx.dtype == torch.long:\n",
    "            if idx.dim() == 0:\n",
    "                return self.graphs[idx], self.line_graphs[idx], self.labels[idx]\n",
    "            elif idx.dim() == 1:\n",
    "                return dgl.data.utils.Subset(self, idx.cpu())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)\n",
    "\n",
    "processed_dataset = ProcessedPCQM4M('./data/molecules_norm.bin', './data/molecules_lg.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = default_timer()\n",
    "\n",
    "# dataset = DglPCQM4MDataset(root='/home/ksadowski/datasets', smiles2graph=smiles2graph)\n",
    "\n",
    "# split_dict = dataset.get_idx_split()\n",
    "\n",
    "# train_idx = split_dict['train']\n",
    "# val_idx = split_dict['valid']\n",
    "# test_idx = split_dict['test']\n",
    "\n",
    "# stop = default_timer()\n",
    "\n",
    "# print(f'Data loading time: {(stop - start) / 60} min.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = torch.load('./data/train_idx.pt')\n",
    "val_idx = torch.load('./data/val_idx.pt')\n",
    "test_idx = torch.load('./data/test_idx.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_ratio = 0.01\n",
    "\n",
    "train_subset = torch.randperm(len(train_idx))[:int(subset_ratio * len(train_idx))]\n",
    "val_subset = torch.randperm(len(val_idx))[:int(subset_ratio * len(val_idx))]\n",
    "test_subset = torch.randperm(len(test_idx))[:int(subset_ratio * len(test_idx))]\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataloader = dgl.dataloading.pytorch.GraphDataLoader(\n",
    "    processed_dataset[train_idx[train_subset]],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "val_dataloader = dgl.dataloading.pytorch.GraphDataLoader(\n",
    "    processed_dataset[val_subset],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "test_dataloader = dgl.dataloading.pytorch.GraphDataLoader(\n",
    "    processed_dataset[test_subset],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "print(f'Train samples: {len(train_dataloader)}')\n",
    "print(f'Val samples: {len(val_dataloader)}')\n",
    "print(f'Test samples: {len(test_dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ProcessedPCQM4M(dgl.data.DGLDataset):\n",
    "#     def __init__(self, ogb_dataset: dgl.data.DGLDataset):\n",
    "#         self.ogb_dataset = ogb_dataset\n",
    "#         self.graphs = []\n",
    "#         self.line_graphs = []\n",
    "#         self.labels = []\n",
    "#         super().__init__(name='processed_PCQM4M')\n",
    "\n",
    "#     def process(self):\n",
    "#         for i in trange(len(self.ogb_dataset)):\n",
    "#             g = self.ogb_dataset[i][0].add_self_loop()\n",
    "#             lg = dgl.line_graph(g, backtracking=False).add_self_loop()\n",
    "\n",
    "#             g.ndata['feat'] = g.ndata['feat'].float()\n",
    "#             g.edata['feat'] = g.edata['feat'].float()\n",
    "\n",
    "#             self.graphs.append(g)\n",
    "#             self.line_graphs.append(lg)\n",
    "#             self.labels.append(self.ogb_dataset[i][1])\n",
    "\n",
    "#     def __getitem__(self, index: Union[int, torch.Tensor]):\n",
    "#         if isinstance(index, int):\n",
    "#             return self.graphs[index], self.line_graphs[index], self.labels[index]\n",
    "#         elif torch.is_tensor(index) and index.dtype == torch.long:\n",
    "#             if index.dim() == 0:\n",
    "#                 return self.graphs[index], self.line_graphs[index], self.labels[index]\n",
    "#             elif index.dim() == 1:\n",
    "#                 return dgl.data.utils.Subset(self, index.cpu())\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.graphs)\n",
    "\n",
    "# processed_dataset = ProcessedPCQM4M(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = {f'{i}': processed_dataset[i][2] for i in range(len(processed_dataset))}\n",
    "\n",
    "# dgl.data.utils.save_graphs('./molecules.bin', processed_dataset.graphs, labels)\n",
    "# dgl.data.utils.save_graphs('./molecules_lg.bin', processed_dataset.line_graphs, labels)"
   ]
  },
  {
   "source": [
    "# Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Transformer"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math\n",
    "# from typing import Tuple\n",
    "\n",
    "# import dgl\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# class LinearLayer(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         in_feats: int,\n",
    "#         out_feats: int,\n",
    "#         activation: str = None,\n",
    "#     ) -> None:\n",
    "#         super().__init__()\n",
    "#         self._linear = nn.Linear(in_feats, out_feats)\n",
    "#         self._activation = activation\n",
    "\n",
    "#     def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "#         x = self._linear(inputs)\n",
    "\n",
    "#         if self._activation == 'relu':\n",
    "#             x = F.relu(x)\n",
    "#         elif self._activation == 'relu6':\n",
    "#             x = F.relu6(x)\n",
    "#         elif self._activation == 'leaky_relu':\n",
    "#             x = F.leaky_relu(x)\n",
    "#         elif self._activation == 'elu':\n",
    "#             x = F.elu(x)\n",
    "#         elif self._activation == 'selu':\n",
    "#             x = F.selu(x)\n",
    "#         elif self._activation == 'celu':\n",
    "#             x = F.celu(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class BilinearReadoutLayer(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         node_in_feats: int,\n",
    "#         edge_in_feats: int,\n",
    "#         out_feats: int,\n",
    "#         activation: str = None,\n",
    "#     ) -> None:\n",
    "#         super().__init__()\n",
    "#         self._bilinear = nn.Bilinear(node_in_feats, edge_in_feats, out_feats)\n",
    "#         self._activation = activation\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         node_inputs: torch.Tensor,\n",
    "#         edge_inputs: torch.Tensor,\n",
    "#     ) -> float:\n",
    "#         x = self._bilinear(node_inputs, edge_inputs)\n",
    "\n",
    "#         if self._activation == 'relu':\n",
    "#             x = F.relu(x)\n",
    "#         elif self._activation == 'softplus':\n",
    "#             x = F.softplus(x)\n",
    "\n",
    "#         return x\n",
    "\n",
    "\n",
    "# class MutualMultiAttentionHead(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         node_in_feats: int,\n",
    "#         edge_in_feats: int,\n",
    "#         num_heads: int,\n",
    "#         short_residual: bool,\n",
    "#         dropout_probability: float,\n",
    "#         message_aggregation_type: str,\n",
    "#         head_pooling_type: str,\n",
    "#         linear_projection_activation: str = None,\n",
    "#     ) -> None:\n",
    "#         super().__init__()\n",
    "#         self._node_in_feats = node_in_feats\n",
    "#         self._edge_in_feats = edge_in_feats\n",
    "#         self._num_heads = num_heads\n",
    "#         self._short_residual = short_residual\n",
    "#         self._message_aggregation_type = message_aggregation_type\n",
    "#         self._head_pooling_type = head_pooling_type\n",
    "#         self._device = nn.Parameter(torch.empty(0))\n",
    "#         self._node_query_linear = LinearLayer(\n",
    "#             node_in_feats,\n",
    "#             num_heads * node_in_feats,\n",
    "#             linear_projection_activation,\n",
    "#         )\n",
    "#         self._node_key_linear = LinearLayer(\n",
    "#             node_in_feats, num_heads, linear_projection_activation)\n",
    "#         self._node_value_linear = LinearLayer(\n",
    "#             node_in_feats,\n",
    "#             num_heads * node_in_feats,\n",
    "#             linear_projection_activation,\n",
    "#         )\n",
    "#         self._edge_query_linear = LinearLayer(\n",
    "#             edge_in_feats,\n",
    "#             num_heads * edge_in_feats,\n",
    "#             linear_projection_activation,\n",
    "#         )\n",
    "#         self._edge_key_linear = LinearLayer(\n",
    "#             edge_in_feats, num_heads, linear_projection_activation)\n",
    "#         self._edge_value_linear = LinearLayer(\n",
    "#             edge_in_feats,\n",
    "#             num_heads * edge_in_feats,\n",
    "#             linear_projection_activation,\n",
    "#         )\n",
    "#         self._node_dropout = nn.Dropout(dropout_probability)\n",
    "#         self._edge_dropout = nn.Dropout(dropout_probability)\n",
    "\n",
    "#     def _calculate_self_attention(\n",
    "#         self,\n",
    "#         query: torch.Tensor,\n",
    "#         key: torch.Tensor,\n",
    "#         in_feats: int,\n",
    "#         short_residual: torch.Tensor = None,\n",
    "#     ) -> torch.Tensor:\n",
    "#         if short_residual is not None:\n",
    "#             x = query @ torch.transpose(short_residual, -1, -2) @ key\n",
    "#         else:\n",
    "#             x = query @ torch.transpose(query, -1, -2) @ key\n",
    "\n",
    "#         x /= math.sqrt(in_feats)\n",
    "#         x = F.softmax(x, dim=1)\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     def _create_node_attention_projection(\n",
    "#         self,\n",
    "#         g: dgl.DGLGraph,\n",
    "#         edge_self_attention: torch.Tensor,\n",
    "#     ) -> torch.Tensor:\n",
    "#         node_attention_projection = torch.zeros(\n",
    "#             [self._num_heads, g.num_nodes(), g.num_nodes()],\n",
    "#             device=self._device.device,\n",
    "#         )\n",
    "\n",
    "#         for edge in range(g.num_edges()):\n",
    "#             nodes = g.find_edges(edge)\n",
    "\n",
    "#             source = nodes[0].item()\n",
    "#             destination = nodes[1].item()\n",
    "\n",
    "#             for head in range(self._num_heads):\n",
    "#                 attention_score = edge_self_attention[head][edge]\n",
    "\n",
    "#                 node_attention_projection[head][source][destination] = attention_score\n",
    "\n",
    "#             return node_attention_projection\n",
    "\n",
    "#     def _create_edge_attention_projection(\n",
    "#         self,\n",
    "#         g: dgl.DGLGraph,\n",
    "#         lg: dgl.DGLGraph,\n",
    "#         node_self_attention: torch.Tensor,\n",
    "#     ) -> torch.Tensor:\n",
    "#         edge_attention_projection = torch.zeros(\n",
    "#             [self._num_heads, g.num_edges(), g.num_edges()],\n",
    "#             device=self._device.device,\n",
    "#         )\n",
    "\n",
    "#         for node in range(lg.num_edges()):\n",
    "#             edges = lg.find_edges(node)\n",
    "\n",
    "#             source = edges[0].item()\n",
    "#             destination = edges[1].item()\n",
    "\n",
    "#             connecting_node = g.find_edges(source)[1].item()\n",
    "\n",
    "#             for head in range(self._num_heads):\n",
    "#                 attention_score = node_self_attention[head][connecting_node]\n",
    "\n",
    "#                 edge_attention_projection[head][source][destination] = attention_score\n",
    "\n",
    "#             return edge_attention_projection\n",
    "\n",
    "#     def _calculate_message_passing(\n",
    "#         self,\n",
    "#         g: dgl.DGLGraph,\n",
    "#         value: torch.Tensor,\n",
    "#         attention_projection: torch.Tensor,\n",
    "#     ):\n",
    "#         adjacency = g.adj(ctx=self._device.device).to_dense()\n",
    "\n",
    "#         if self._message_aggregation_type == 'sum':\n",
    "#             x = attention_projection * adjacency\n",
    "#         elif self._message_aggregation_type == 'mean':\n",
    "#             degree_inv = torch.linalg.inv(torch.diag(g.in_degrees().float()))\n",
    "\n",
    "#             x = degree_inv @ attention_projection * adjacency\n",
    "#         elif self._message_aggregation_type == 'gcn':\n",
    "#             degree_inv_sqrt = torch.sqrt(torch.linalg.inv(\n",
    "#                 torch.diag(g.in_degrees().float())))\n",
    "#             adjacency_inv_sqrt = torch.sqrt(torch.linalg.inv(adjacency))\n",
    "\n",
    "#             x = degree_inv_sqrt @ attention_projection * \\\n",
    "#                 adjacency_inv_sqrt @ degree_inv_sqrt\n",
    "\n",
    "#         message_passing = x @ value\n",
    "\n",
    "#         return message_passing\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         g: dgl.DGLGraph,\n",
    "#         lg: dgl.DGLGraph,\n",
    "#         node_inputs: torch.Tensor,\n",
    "#         edge_inputs: torch.Tensor,\n",
    "#     ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#         node_query = self._node_query_linear(node_inputs)\n",
    "#         node_query = node_query.view(self._num_heads, -1, self._node_in_feats)\n",
    "#         node_key = self._node_key_linear(node_inputs)\n",
    "#         node_key = node_key.view(self._num_heads, -1, 1)\n",
    "#         node_value = self._node_value_linear(node_inputs)\n",
    "#         node_value = node_value.view(self._num_heads, -1, self._node_in_feats)\n",
    "\n",
    "#         edge_query = self._edge_query_linear(edge_inputs)\n",
    "#         edge_query = edge_query.view(self._num_heads, -1, self._edge_in_feats)\n",
    "#         edge_key = self._edge_key_linear(edge_inputs)\n",
    "#         edge_key = edge_key.view(self._num_heads, -1, 1)\n",
    "#         edge_value = self._edge_value_linear(edge_inputs)\n",
    "#         edge_value = edge_value.view(self._num_heads, -1, self._edge_in_feats)\n",
    "\n",
    "#         if self._short_residual:\n",
    "#             node_self_attention = self._calculate_self_attention(\n",
    "#                 node_query, node_key, self._node_in_feats, node_inputs)\n",
    "#             edge_self_attention = self._calculate_self_attention(\n",
    "#                 edge_query, edge_key, self._edge_in_feats, edge_inputs)\n",
    "#         else:\n",
    "#             node_self_attention = self._calculate_self_attention(\n",
    "#                 node_query, node_key, self._node_in_feats)\n",
    "#             edge_self_attention = self._calculate_self_attention(\n",
    "#                 edge_query, edge_key, self._edge_in_feats)\n",
    "\n",
    "#         node_attention_projection = self._create_node_attention_projection(\n",
    "#             g, edge_self_attention)\n",
    "#         edge_attention_projection = self._create_edge_attention_projection(\n",
    "#             g, lg, node_self_attention)\n",
    "\n",
    "#         node_message_passing = self._calculate_message_passing(\n",
    "#             g, node_value, node_attention_projection)\n",
    "#         edge_message_passing = self._calculate_message_passing(\n",
    "#             lg, edge_value, edge_attention_projection)\n",
    "\n",
    "#         node_message_passing = self._node_dropout(node_message_passing)\n",
    "#         edge_message_passing = self._edge_dropout(edge_message_passing)\n",
    "\n",
    "#         if self._head_pooling_type == 'sum':\n",
    "#             node_message_passing = node_message_passing.sum(dim=-3)\n",
    "#             edge_message_passing = edge_message_passing.sum(dim=-3)\n",
    "#         elif self._head_pooling_type == 'mean':\n",
    "#             node_message_passing = node_message_passing.mean(dim=-3)\n",
    "#             edge_message_passing = edge_message_passing.mean(dim=-3)\n",
    "\n",
    "#         return node_message_passing, edge_message_passing\n",
    "\n",
    "\n",
    "# class MutualAttentionTransformerLayer(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         node_in_feats: int,\n",
    "#         node_out_feats: int,\n",
    "#         edge_in_feats: int,\n",
    "#         edge_out_feats: int,\n",
    "#         num_heads: int,\n",
    "#         short_residual: bool,\n",
    "#         long_residual: bool,\n",
    "#         dropout_probability: float,\n",
    "#         message_aggregation_type: str,\n",
    "#         head_pooling_type: str,\n",
    "#         normalization_type: str,\n",
    "#         linear_projection_activation: str = None,\n",
    "#         linear_embedding_activation: str = None,\n",
    "#     ) -> None:\n",
    "#         super().__init__()\n",
    "#         self._long_residual = long_residual\n",
    "#         self._mutual_multi_attention_head = MutualMultiAttentionHead(\n",
    "#             node_in_feats,\n",
    "#             edge_in_feats,\n",
    "#             num_heads,\n",
    "#             short_residual,\n",
    "#             dropout_probability,\n",
    "#             message_aggregation_type,\n",
    "#             head_pooling_type,\n",
    "#             linear_projection_activation,\n",
    "#         )\n",
    "#         self._node_linear_embedding = LinearLayer(\n",
    "#             node_in_feats, node_out_feats, linear_embedding_activation)\n",
    "#         self._edge_linear_embedding = LinearLayer(\n",
    "#             edge_in_feats, edge_out_feats, linear_embedding_activation)\n",
    "\n",
    "#         if normalization_type == 'layer':\n",
    "#             self._node_normalization_1 = nn.LayerNorm(node_in_feats)\n",
    "#             self._node_normalization_2 = nn.LayerNorm(node_out_feats)\n",
    "\n",
    "#             self._edge_normalization_1 = nn.LayerNorm(edge_in_feats)\n",
    "#             self._edge_normalization_2 = nn.LayerNorm(edge_out_feats)\n",
    "#         elif normalization_type == 'batch':\n",
    "#             self._node_normalization_1 = nn.BatchNorm1d(node_in_feats)\n",
    "#             self._node_normalization_2 = nn.BatchNorm1d(node_out_feats)\n",
    "\n",
    "#             self._edge_normalization_1 = nn.BatchNorm1d(edge_in_feats)\n",
    "#             self._edge_normalization_2 = nn.BatchNorm1d(edge_out_feats)\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         g: dgl.DGLGraph,\n",
    "#         lg: dgl.DGLGraph,\n",
    "#         node_inputs: torch.Tensor,\n",
    "#         edge_inputs: torch.Tensor,\n",
    "#     ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#         node_embedding, edge_embedding = self._mutual_multi_attention_head(\n",
    "#             g, lg, node_inputs, edge_inputs)\n",
    "\n",
    "#         if self._long_residual:\n",
    "#             node_embedding += node_inputs\n",
    "#             edge_embedding += edge_inputs\n",
    "\n",
    "#         node_embedding = self._node_normalization_1(node_embedding)\n",
    "#         edge_embedding = self._edge_normalization_1(edge_embedding)\n",
    "\n",
    "#         node_embedding = self._node_linear_embedding(node_embedding)\n",
    "#         edge_embedding = self._edge_linear_embedding(edge_embedding)\n",
    "\n",
    "#         node_embedding = self._node_normalization_2(node_embedding)\n",
    "#         edge_embedding = self._edge_normalization_2(edge_embedding)\n",
    "\n",
    "#         return node_embedding, edge_embedding\n",
    "\n",
    "\n",
    "# class GraphMutualAttentionTransformer(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         node_in_feats: int,\n",
    "#         node_hidden_feats: int,\n",
    "#         node_out_feats: int,\n",
    "#         edge_in_feats: int,\n",
    "#         edge_hidden_feats: int,\n",
    "#         edge_out_feats: int,\n",
    "#         num_layers: int,\n",
    "#         num_heads: int,\n",
    "#         short_residual: bool,\n",
    "#         long_residual: bool,\n",
    "#         dropout_probability: float,\n",
    "#         message_aggregation_type: str,\n",
    "#         head_pooling_type: str,\n",
    "#         readout_pooling_type: str,\n",
    "#         normalization_type: str,\n",
    "#         linear_projection_activation: str = None,\n",
    "#         linear_embedding_activation: str = None,\n",
    "#         bilinear_readout_activation: str = None,\n",
    "#     ) -> None:\n",
    "#         super().__init__()\n",
    "#         self._node_out_feats = node_out_feats\n",
    "#         self._edge_out_feats = edge_out_feats\n",
    "#         self._num_layers = num_layers\n",
    "#         self._transformer_layers = self._create_transformer_layers(\n",
    "#             node_in_feats,\n",
    "#             node_hidden_feats,\n",
    "#             node_out_feats,\n",
    "#             edge_in_feats,\n",
    "#             edge_hidden_feats,\n",
    "#             edge_out_feats,\n",
    "#             num_layers,\n",
    "#             num_heads,\n",
    "#             short_residual,\n",
    "#             long_residual,\n",
    "#             dropout_probability,\n",
    "#             message_aggregation_type,\n",
    "#             head_pooling_type,\n",
    "#             normalization_type,\n",
    "#             linear_projection_activation,\n",
    "#             linear_embedding_activation,\n",
    "#         )\n",
    "#         self._bilinear_readout = BilinearReadoutLayer(\n",
    "#             node_out_feats, edge_out_feats, 1, bilinear_readout_activation)\n",
    "\n",
    "#         if readout_pooling_type == 'sum':\n",
    "#             self._readout_pooling = dgl.nn.pytorch.SumPooling()\n",
    "#         elif readout_pooling_type == 'mean':\n",
    "#             self._readout_pooling = dgl.nn.pytorch.AvgPooling()\n",
    "#         elif readout_pooling_type == 'attention':\n",
    "#             pass\n",
    "\n",
    "#     def _create_transformer_layers(\n",
    "#         self,\n",
    "#         node_in_feats: int,\n",
    "#         node_hidden_feats: int,\n",
    "#         node_out_feats: int,\n",
    "#         edge_in_feats: int,\n",
    "#         edge_hidden_feats: int,\n",
    "#         edge_out_feats: int,\n",
    "#         num_layers: int,\n",
    "#         num_heads: int,\n",
    "#         short_residual: bool,\n",
    "#         long_residual: bool,\n",
    "#         dropout_probability: float,\n",
    "#         message_aggregation_type: str,\n",
    "#         head_pooling_type: str,\n",
    "#         normalization_type: str,\n",
    "#         linear_projection_activation: str = None,\n",
    "#         linear_embedding_activation: str = None,\n",
    "#     ) -> nn.ModuleList:\n",
    "#         transformer_layers = nn.ModuleList()\n",
    "\n",
    "#         if num_layers > 1:\n",
    "#             transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "#                 node_in_feats,\n",
    "#                 node_hidden_feats,\n",
    "#                 edge_in_feats,\n",
    "#                 edge_hidden_feats,\n",
    "#                 num_heads,\n",
    "#                 short_residual,\n",
    "#                 long_residual,\n",
    "#                 dropout_probability,\n",
    "#                 message_aggregation_type,\n",
    "#                 head_pooling_type,\n",
    "#                 normalization_type,\n",
    "#                 linear_projection_activation,\n",
    "#                 linear_embedding_activation,\n",
    "#             ))\n",
    "\n",
    "#             for _ in range(num_layers - 2):\n",
    "#                 transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "#                     node_hidden_feats,\n",
    "#                     node_hidden_feats,\n",
    "#                     edge_hidden_feats,\n",
    "#                     edge_hidden_feats,\n",
    "#                     num_heads,\n",
    "#                     short_residual,\n",
    "#                     long_residual,\n",
    "#                     dropout_probability,\n",
    "#                     message_aggregation_type,\n",
    "#                     head_pooling_type,\n",
    "#                     normalization_type,\n",
    "#                     linear_projection_activation,\n",
    "#                     linear_embedding_activation,\n",
    "#                 ))\n",
    "\n",
    "#             transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "#                 node_hidden_feats,\n",
    "#                 node_out_feats,\n",
    "#                 edge_hidden_feats,\n",
    "#                 edge_out_feats,\n",
    "#                 num_heads,\n",
    "#                 short_residual,\n",
    "#                 long_residual,\n",
    "#                 dropout_probability,\n",
    "#                 message_aggregation_type,\n",
    "#                 head_pooling_type,\n",
    "#                 normalization_type,\n",
    "#                 linear_projection_activation,\n",
    "#                 linear_embedding_activation,\n",
    "#             ))\n",
    "#         else:\n",
    "#             transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "#                 node_in_feats,\n",
    "#                 node_out_feats,\n",
    "#                 edge_in_feats,\n",
    "#                 edge_out_feats,\n",
    "#                 num_heads,\n",
    "#                 short_residual,\n",
    "#                 long_residual,\n",
    "#                 dropout_probability,\n",
    "#                 message_aggregation_type,\n",
    "#                 head_pooling_type,\n",
    "#                 normalization_type,\n",
    "#                 linear_projection_activation,\n",
    "#                 linear_embedding_activation,\n",
    "#             ))\n",
    "\n",
    "#         return transformer_layers\n",
    "\n",
    "#     def forward(\n",
    "#         self,\n",
    "#         g: dgl.DGLGraph,\n",
    "#         lg: dgl.DGLGraph,\n",
    "#         node_inputs: torch.Tensor,\n",
    "#         edge_inputs: torch.Tensor,\n",
    "#     ) -> torch.Tensor:\n",
    "#         node_embedding = node_inputs\n",
    "#         edge_embedding = edge_inputs\n",
    "\n",
    "#         for transformer_layer in self._transformer_layers:\n",
    "#             node_embedding, edge_embedding = transformer_layer(\n",
    "#                 g, lg, node_inputs, edge_inputs)\n",
    "        \n",
    "#         node_embedding = self._readout_pooling(g, node_embedding)\n",
    "#         edge_embedding = self._readout_pooling(lg, edge_embedding)\n",
    "\n",
    "#         readout = self._bilinear_readout(node_embedding, edge_embedding)\n",
    "\n",
    "#         return readout"
   ]
  },
  {
   "source": [
    "## Simple"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LinearLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feats: int,\n",
    "        out_feats: int,\n",
    "        activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._linear = nn.Linear(in_feats, out_feats)\n",
    "        self._activation = activation\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        x = self._linear(inputs)\n",
    "\n",
    "        if self._activation == 'relu':\n",
    "            x = F.relu(x)\n",
    "        elif self._activation == 'relu6':\n",
    "            x = F.relu6(x)\n",
    "        elif self._activation == 'leaky_relu':\n",
    "            x = F.leaky_relu(x)\n",
    "        elif self._activation == 'elu':\n",
    "            x = F.elu(x)\n",
    "        elif self._activation == 'selu':\n",
    "            x = F.selu(x)\n",
    "        elif self._activation == 'celu':\n",
    "            x = F.celu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BilinearReadoutLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        out_feats: int,\n",
    "        activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._bilinear = nn.Bilinear(node_in_feats, edge_in_feats, out_feats)\n",
    "        self._activation = activation\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> float:\n",
    "        x = self._bilinear(node_inputs, edge_inputs)\n",
    "\n",
    "        if self._activation == 'relu':\n",
    "            x = F.relu(x)\n",
    "        elif self._activation == 'softplus':\n",
    "            x = F.softplus(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MutualMultiAttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        num_heads: int,\n",
    "        dropout_probability: float,\n",
    "        message_aggregation_type: str,\n",
    "        head_pooling_type: str,\n",
    "        linear_projection_activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._node_in_feats = node_in_feats\n",
    "        self._edge_in_feats = edge_in_feats\n",
    "        self._num_heads = num_heads\n",
    "        self._message_aggregation_type = message_aggregation_type\n",
    "        self._head_pooling_type = head_pooling_type\n",
    "        self._device = nn.Parameter(torch.empty(0))\n",
    "        self._node_key_linear = LinearLayer(\n",
    "            node_in_feats, num_heads, linear_projection_activation)\n",
    "        self._node_value_linear = LinearLayer(\n",
    "            node_in_feats,\n",
    "            num_heads * node_in_feats,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "        self._edge_key_linear = LinearLayer(\n",
    "            edge_in_feats, num_heads, linear_projection_activation)\n",
    "        self._edge_value_linear = LinearLayer(\n",
    "            edge_in_feats,\n",
    "            num_heads * edge_in_feats,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "        self._node_dropout = nn.Dropout(dropout_probability)\n",
    "        self._edge_dropout = nn.Dropout(dropout_probability)\n",
    "\n",
    "    def _calculate_self_attention(\n",
    "        self,\n",
    "        key: torch.Tensor,\n",
    "        in_feats: int,\n",
    "    ) -> torch.Tensor:\n",
    "        x = key / math.sqrt(in_feats)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _create_node_attention_projection(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        edge_self_attention: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        node_attention_projection = torch.zeros(\n",
    "            [self._num_heads, g.num_nodes(), g.num_nodes()],\n",
    "            device=self._device.device,\n",
    "        )\n",
    "\n",
    "        for edge in range(g.num_edges()):\n",
    "            nodes = g.find_edges(edge)\n",
    "\n",
    "            source = nodes[0].item()\n",
    "            destination = nodes[1].item()\n",
    "\n",
    "            for head in range(self._num_heads):\n",
    "                attention_score = edge_self_attention[head][edge]\n",
    "\n",
    "                node_attention_projection[head][source][destination] = attention_score\n",
    "\n",
    "            return node_attention_projection\n",
    "\n",
    "    def _create_edge_attention_projection(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        lg: dgl.DGLGraph,\n",
    "        node_self_attention: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        edge_attention_projection = torch.zeros(\n",
    "            [self._num_heads, g.num_edges(), g.num_edges()],\n",
    "            device=self._device.device,\n",
    "        )\n",
    "\n",
    "        for node in range(lg.num_edges()):\n",
    "            edges = lg.find_edges(node)\n",
    "\n",
    "            source = edges[0].item()\n",
    "            destination = edges[1].item()\n",
    "\n",
    "            connecting_node = g.find_edges(source)[1].item()\n",
    "\n",
    "            for head in range(self._num_heads):\n",
    "                attention_score = node_self_attention[head][connecting_node]\n",
    "\n",
    "                edge_attention_projection[head][source][destination] = attention_score\n",
    "\n",
    "            return edge_attention_projection\n",
    "\n",
    "    def _calculate_message_passing(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        value: torch.Tensor,\n",
    "        attention_projection: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        adjacency = g.adj(ctx=self._device.device).to_dense()\n",
    "\n",
    "        if self._message_aggregation_type == 'sum':\n",
    "            x = attention_projection * adjacency\n",
    "        elif self._message_aggregation_type == 'mean':\n",
    "            degree_inv = torch.linalg.inv(torch.diag(g.in_degrees().float()))\n",
    "\n",
    "            x = degree_inv @ attention_projection * adjacency\n",
    "        elif self._message_aggregation_type == 'gcn':\n",
    "            degree_inv_sqrt = torch.sqrt(torch.linalg.inv(\n",
    "                torch.diag(g.in_degrees().float())))\n",
    "            adjacency_inv_sqrt = torch.sqrt(torch.linalg.inv(adjacency))\n",
    "\n",
    "            x = degree_inv_sqrt @ attention_projection * \\\n",
    "                adjacency_inv_sqrt @ degree_inv_sqrt\n",
    "\n",
    "        message_passing = x @ value\n",
    "\n",
    "        return message_passing\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        lg: dgl.DGLGraph,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        node_key = self._node_key_linear(node_inputs)\n",
    "        node_key = node_key.view(self._num_heads, -1, 1)\n",
    "        node_value = self._node_value_linear(node_inputs)\n",
    "        node_value = node_value.view(self._num_heads, -1, self._node_in_feats)\n",
    "\n",
    "        edge_key = self._edge_key_linear(edge_inputs)\n",
    "        edge_key = edge_key.view(self._num_heads, -1, 1)\n",
    "        edge_value = self._edge_value_linear(edge_inputs)\n",
    "        edge_value = edge_value.view(self._num_heads, -1, self._edge_in_feats)\n",
    "\n",
    "\n",
    "        node_self_attention = self._calculate_self_attention(\n",
    "            node_key, self._node_in_feats)\n",
    "        edge_self_attention = self._calculate_self_attention(\n",
    "            edge_key, self._edge_in_feats)\n",
    "\n",
    "        node_attention_projection = self._create_node_attention_projection(\n",
    "            g, edge_self_attention)\n",
    "        edge_attention_projection = self._create_edge_attention_projection(\n",
    "            g, lg, node_self_attention)\n",
    "\n",
    "        node_message_passing = self._calculate_message_passing(\n",
    "            g, node_value, node_attention_projection)\n",
    "        edge_message_passing = self._calculate_message_passing(\n",
    "            lg, edge_value, edge_attention_projection)\n",
    "\n",
    "        node_message_passing = self._node_dropout(node_message_passing)\n",
    "        edge_message_passing = self._edge_dropout(edge_message_passing)\n",
    "\n",
    "        if self._head_pooling_type == 'sum':\n",
    "            node_message_passing = node_message_passing.sum(dim=-3)\n",
    "            edge_message_passing = edge_message_passing.sum(dim=-3)\n",
    "        elif self._head_pooling_type == 'mean':\n",
    "            node_message_passing = node_message_passing.mean(dim=-3)\n",
    "            edge_message_passing = edge_message_passing.mean(dim=-3)\n",
    "\n",
    "        return node_message_passing, edge_message_passing\n",
    "\n",
    "\n",
    "class MutualAttentionTransformerLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        node_out_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        edge_out_feats: int,\n",
    "        num_heads: int,\n",
    "        long_residual: bool,\n",
    "        dropout_probability: float,\n",
    "        message_aggregation_type: str,\n",
    "        head_pooling_type: str,\n",
    "        normalization_type: str,\n",
    "        linear_projection_activation: str = None,\n",
    "        linear_embedding_activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._long_residual = long_residual\n",
    "        self._mutual_multi_attention_head = MutualMultiAttentionHead(\n",
    "            node_in_feats,\n",
    "            edge_in_feats,\n",
    "            num_heads,\n",
    "            dropout_probability,\n",
    "            message_aggregation_type,\n",
    "            head_pooling_type,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "        self._node_linear_embedding = LinearLayer(\n",
    "            node_in_feats, node_out_feats, linear_embedding_activation)\n",
    "        self._edge_linear_embedding = LinearLayer(\n",
    "            edge_in_feats, edge_out_feats, linear_embedding_activation)\n",
    "\n",
    "        if normalization_type == 'layer':\n",
    "            self._node_normalization_1 = nn.LayerNorm(node_in_feats)\n",
    "            self._node_normalization_2 = nn.LayerNorm(node_out_feats)\n",
    "\n",
    "            self._edge_normalization_1 = nn.LayerNorm(edge_in_feats)\n",
    "            self._edge_normalization_2 = nn.LayerNorm(edge_out_feats)\n",
    "        elif normalization_type == 'batch':\n",
    "            self._node_normalization_1 = nn.BatchNorm1d(node_in_feats)\n",
    "            self._node_normalization_2 = nn.BatchNorm1d(node_out_feats)\n",
    "\n",
    "            self._edge_normalization_1 = nn.BatchNorm1d(edge_in_feats)\n",
    "            self._edge_normalization_2 = nn.BatchNorm1d(edge_out_feats)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        lg: dgl.DGLGraph,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        node_embedding, edge_embedding = self._mutual_multi_attention_head(\n",
    "            g, lg, node_inputs, edge_inputs)\n",
    "\n",
    "        if self._long_residual:\n",
    "            node_embedding += node_inputs\n",
    "            edge_embedding += edge_inputs\n",
    "\n",
    "        node_embedding = self._node_normalization_1(node_embedding)\n",
    "        edge_embedding = self._edge_normalization_1(edge_embedding)\n",
    "\n",
    "        node_embedding = self._node_linear_embedding(node_embedding)\n",
    "        edge_embedding = self._edge_linear_embedding(edge_embedding)\n",
    "\n",
    "        node_embedding = self._node_normalization_2(node_embedding)\n",
    "        edge_embedding = self._edge_normalization_2(edge_embedding)\n",
    "\n",
    "        return node_embedding, edge_embedding\n",
    "\n",
    "\n",
    "class GraphMutualAttentionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        node_hidden_feats: int,\n",
    "        node_out_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        edge_hidden_feats: int,\n",
    "        edge_out_feats: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        long_residual: bool,\n",
    "        dropout_probability: float,\n",
    "        message_aggregation_type: str,\n",
    "        head_pooling_type: str,\n",
    "        readout_pooling_type: str,\n",
    "        normalization_type: str,\n",
    "        linear_projection_activation: str = None,\n",
    "        linear_embedding_activation: str = None,\n",
    "        bilinear_readout_activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._node_out_feats = node_out_feats\n",
    "        self._edge_out_feats = edge_out_feats\n",
    "        self._num_layers = num_layers\n",
    "        self._transformer_layers = self._create_transformer_layers(\n",
    "            node_in_feats,\n",
    "            node_hidden_feats,\n",
    "            node_out_feats,\n",
    "            edge_in_feats,\n",
    "            edge_hidden_feats,\n",
    "            edge_out_feats,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            long_residual,\n",
    "            dropout_probability,\n",
    "            message_aggregation_type,\n",
    "            head_pooling_type,\n",
    "            normalization_type,\n",
    "            linear_projection_activation,\n",
    "            linear_embedding_activation,\n",
    "        )\n",
    "        self._bilinear_readout = BilinearReadoutLayer(\n",
    "            node_out_feats, edge_out_feats, 1, bilinear_readout_activation)\n",
    "\n",
    "        if readout_pooling_type == 'sum':\n",
    "            self._readout_pooling = dgl.nn.pytorch.SumPooling()\n",
    "        elif readout_pooling_type == 'mean':\n",
    "            self._readout_pooling = dgl.nn.pytorch.AvgPooling()\n",
    "        elif readout_pooling_type == 'attention':\n",
    "            pass\n",
    "\n",
    "    def _create_transformer_layers(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        node_hidden_feats: int,\n",
    "        node_out_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        edge_hidden_feats: int,\n",
    "        edge_out_feats: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        long_residual: bool,\n",
    "        dropout_probability: float,\n",
    "        message_aggregation_type: str,\n",
    "        head_pooling_type: str,\n",
    "        normalization_type: str,\n",
    "        linear_projection_activation: str = None,\n",
    "        linear_embedding_activation: str = None,\n",
    "    ) -> nn.ModuleList:\n",
    "        transformer_layers = nn.ModuleList()\n",
    "\n",
    "        if num_layers > 1:\n",
    "            transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                node_in_feats,\n",
    "                node_hidden_feats,\n",
    "                edge_in_feats,\n",
    "                edge_hidden_feats,\n",
    "                num_heads,\n",
    "                long_residual,\n",
    "                dropout_probability,\n",
    "                message_aggregation_type,\n",
    "                head_pooling_type,\n",
    "                normalization_type,\n",
    "                linear_projection_activation,\n",
    "                linear_embedding_activation,\n",
    "            ))\n",
    "\n",
    "            for _ in range(num_layers - 2):\n",
    "                transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                    node_hidden_feats,\n",
    "                    node_hidden_feats,\n",
    "                    edge_hidden_feats,\n",
    "                    edge_hidden_feats,\n",
    "                    num_heads,\n",
    "                    long_residual,\n",
    "                    dropout_probability,\n",
    "                    message_aggregation_type,\n",
    "                    head_pooling_type,\n",
    "                    normalization_type,\n",
    "                    linear_projection_activation,\n",
    "                    linear_embedding_activation,\n",
    "                ))\n",
    "\n",
    "            transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                node_hidden_feats,\n",
    "                node_out_feats,\n",
    "                edge_hidden_feats,\n",
    "                edge_out_feats,\n",
    "                num_heads,\n",
    "                long_residual,\n",
    "                dropout_probability,\n",
    "                message_aggregation_type,\n",
    "                head_pooling_type,\n",
    "                normalization_type,\n",
    "                linear_projection_activation,\n",
    "                linear_embedding_activation,\n",
    "            ))\n",
    "        else:\n",
    "            transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                node_in_feats,\n",
    "                node_out_feats,\n",
    "                edge_in_feats,\n",
    "                edge_out_feats,\n",
    "                num_heads,\n",
    "                long_residual,\n",
    "                dropout_probability,\n",
    "                message_aggregation_type,\n",
    "                head_pooling_type,\n",
    "                normalization_type,\n",
    "                linear_projection_activation,\n",
    "                linear_embedding_activation,\n",
    "            ))\n",
    "\n",
    "        return transformer_layers\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        lg: dgl.DGLGraph,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        node_embedding = node_inputs\n",
    "        edge_embedding = edge_inputs\n",
    "\n",
    "        for transformer_layer in self._transformer_layers:\n",
    "            node_embedding, edge_embedding = transformer_layer(\n",
    "                g, lg, node_inputs, edge_inputs)\n",
    "        \n",
    "        node_embedding = self._readout_pooling(g, node_embedding)\n",
    "        edge_embedding = self._readout_pooling(lg, edge_embedding)\n",
    "\n",
    "        readout = self._bilinear_readout(node_embedding, edge_embedding)\n",
    "\n",
    "        return readout\n"
   ]
  },
  {
   "source": [
    "# Training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: nn.Module, train_dataloader, val_dataloader, test_dataloader, num_epochs: int, device: str) -> None:\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    for epoch in range(1, 1 + num_epochs):\n",
    "        start = default_timer()\n",
    "\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "\n",
    "        # training\n",
    "        model.train()\n",
    "\n",
    "        for step, (batched_g, batched_lg, labels) in enumerate(train_dataloader):\n",
    "            batched_g = batched_g.to(device)\n",
    "            batched_lg = batched_lg.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            node_inputs = batched_g.ndata.pop('feat')\n",
    "            edge_inputs = batched_g.edata.pop('feat')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            pred = model(batched_g, batched_lg, node_inputs, edge_inputs).view(-1,)\n",
    "\n",
    "            loss = torch.nn.L1Loss()(pred, labels)\n",
    "            train_loss += loss\n",
    "\n",
    "            # print(labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss /= len(train_dataloader)\n",
    "\n",
    "        # validation\n",
    "        # if epoch % 10 == 0:\n",
    "        model.eval()\n",
    "\n",
    "        for step, (batched_g, batched_lg, labels) in enumerate(val_dataloader):\n",
    "            batched_g = batched_g.to(device)\n",
    "            batched_lg = batched_lg.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            node_inputs = batched_g.ndata.pop('feat')\n",
    "            edge_inputs = batched_g.edata.pop('feat')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred = model(batched_g, batched_lg, node_inputs, edge_inputs).view(-1,)\n",
    "\n",
    "            loss = torch.nn.L1Loss()(pred, labels)\n",
    "            val_loss += loss\n",
    "        \n",
    "        val_loss /= len(val_dataloader)\n",
    "        \n",
    "        stop = default_timer()\n",
    "\n",
    "        print(f'Epoch: {epoch:3} Train loss: {train_loss:.2f} Validation loss: {val_loss:.2f} Epoch time: {stop - start:.2f}')\n",
    "        # else:\n",
    "        #     stop = default_timer()\n",
    "\n",
    "        #     print(f'Epoch: {epoch:3} Train loss: {train_loss:.2f} Epoch time: {stop - start:.2f}')\n",
    "\n",
    "    # # test\n",
    "    # model.eval()\n",
    "\n",
    "    # for step, (batched_g, batched_lg, labels) in enumerate(test_loader):\n",
    "    #     batched_g = batched_g.to(device)\n",
    "    #     batched_lg = batched_lg.to(device)\n",
    "    #     labels = labels.to(device)\n",
    "\n",
    "    #     node_inputs = batched_g.ndata.pop('feat')\n",
    "    #     edge_inputs = batched_g.edata.pop('feat')\n",
    "\n",
    "    #     optimizer.zero_grad()\n",
    "\n",
    "    #     pred = model(batched_g, batched_lg, node_inputs, edge_inputs).view(-1,)\n",
    "\n",
    "    #     loss = F.l1_loss(pred, labels)\n",
    "    #     train_loss += loss\n",
    "\n",
    "    #     loss.backward()\n",
    "    #     optimizer.step()\n",
    "\n",
    "    # train_loss /= len(test_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "\n",
    "model = GraphMutualAttentionTransformer(\n",
    "    node_in_feats=9,\n",
    "    node_hidden_feats=9,\n",
    "    node_out_feats=9,\n",
    "    edge_in_feats=3,\n",
    "    edge_hidden_feats=3,\n",
    "    edge_out_feats=3,\n",
    "    num_layers=5,\n",
    "    num_heads=9,\n",
    "    # short_residual=True,\n",
    "    long_residual=True,\n",
    "    dropout_probability=0.01,\n",
    "    message_aggregation_type='sum',\n",
    "    head_pooling_type='sum',\n",
    "    readout_pooling_type='mean',\n",
    "    normalization_type='batch',\n",
    "    linear_projection_activation='relu',\n",
    "    linear_embedding_activation='relu',\n",
    "    bilinear_readout_activation='relu',\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "train(model, train_dataloader, val_dataloader, test_dataloader, 100, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}