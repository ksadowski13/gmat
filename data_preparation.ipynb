{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd04884a46182a36c52786c58ae747f94ef8bde10a8978cfcd06107010b24cc677d",
   "display_name": "Python 3.8.8 64-bit ('ogb-lsc-env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "import dgl\n",
    "\n",
    "from ogb.lsc import DglPCQM4MDataset\n",
    "from ogb.utils import smiles2graph\n",
    "\n",
    "from timeit import default_timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DglPCQM4MDataset(root='/home/ksadowski/datasets', smiles2graph=smiles2graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dict = dataset.get_idx_split()\n",
    "\n",
    "train_idx = split_dict['train']\n",
    "valid_idx = split_dict['valid']\n",
    "test_idx = split_dict['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in test_idx:\n",
    "    if dataset[i][0].num_edges() == 0:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    if dataset[i][0].num_edges() == 0:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg.adj().to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_feats: int,\n",
    "        out_feats: int,\n",
    "        activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._linear = nn.Linear(in_feats, out_feats)\n",
    "        self._activation = activation\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        x = self._linear(inputs)\n",
    "\n",
    "        if self._activation == 'relu':\n",
    "            x = F.relu(x)\n",
    "        elif self._activation == 'relu6':\n",
    "            x = F.relu6(x)\n",
    "        elif self._activation == 'leaky_relu':\n",
    "            x = F.leaky_relu(x)\n",
    "        elif self._activation == 'elu':\n",
    "            x = F.elu(x)\n",
    "        elif self._activation == 'selu':\n",
    "            x = F.selu(x)\n",
    "        elif self._activation == 'celu':\n",
    "            x = F.celu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class BilinearReadoutLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        out_feats: int,\n",
    "        activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._bilinear = nn.Bilinear(node_in_feats, edge_in_feats, out_feats)\n",
    "        self._activation = activation\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> float:\n",
    "        x = self._bilinear(node_inputs, edge_inputs)\n",
    "\n",
    "        if self._activation == 'relu':\n",
    "            x = F.relu(x)\n",
    "        elif self._activation == 'softplus':\n",
    "            x = F.softplus(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MutualMultiAttentionHead(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        num_heads: int,\n",
    "        short_residual: bool,\n",
    "        dropout_probability: float,\n",
    "        message_aggregation_type: str,\n",
    "        head_pooling_type: str,\n",
    "        linear_projection_activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._node_in_feats = node_in_feats\n",
    "        self._edge_in_feats = edge_in_feats\n",
    "        self._num_heads = num_heads\n",
    "        self._short_residual = short_residual\n",
    "        self._message_aggregation_type = message_aggregation_type\n",
    "        self._head_pooling_type = head_pooling_type\n",
    "        self._node_query_linear = LinearLayer(\n",
    "            node_in_feats,\n",
    "            num_heads * node_in_feats,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "        self._node_key_linear = LinearLayer(\n",
    "            node_in_feats, num_heads, linear_projection_activation)\n",
    "        self._node_value_linear = LinearLayer(\n",
    "            node_in_feats,\n",
    "            num_heads * node_in_feats,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "        self._edge_query_linear = LinearLayer(\n",
    "            edge_in_feats,\n",
    "            num_heads * edge_in_feats,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "        self._edge_key_linear = LinearLayer(\n",
    "            edge_in_feats, num_heads, linear_projection_activation)\n",
    "        self._edge_value_linear = LinearLayer(\n",
    "            edge_in_feats,\n",
    "            num_heads * edge_in_feats,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "        self._node_dropout = nn.Dropout(dropout_probability)\n",
    "        self._edge_dropout = nn.Dropout(dropout_probability)\n",
    "\n",
    "    def _calculate_self_attention(\n",
    "        self,\n",
    "        query: torch.Tensor,\n",
    "        key: torch.Tensor,\n",
    "        in_feats: int,\n",
    "        short_residual: torch.Tensor = None,\n",
    "    ) -> torch.Tensor:\n",
    "        if short_residual is not None:\n",
    "            x = query @ torch.transpose(short_residual, -1, -2) @ key\n",
    "        else:\n",
    "            x = query @ torch.transpose(query, -1, -2) @ key\n",
    "\n",
    "        x /= math.sqrt(in_feats)\n",
    "        x = F.softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def _create_node_attention_projection(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        edge_self_attention: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        node_attention_projection = torch.zeros(\n",
    "            [self._num_heads, g.num_nodes(), g.num_nodes()])\n",
    "\n",
    "        for edge in range(g.num_edges()):\n",
    "            nodes = g.find_edges(edge)\n",
    "\n",
    "            source = nodes[0].item()\n",
    "            destination = nodes[1].item()\n",
    "\n",
    "            for head in range(self._num_heads):\n",
    "                attention_score = edge_self_attention[head][edge]\n",
    "\n",
    "                node_attention_projection[head][source][destination] = attention_score\n",
    "\n",
    "            return node_attention_projection\n",
    "\n",
    "    def _create_edge_attention_projection(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        lg: dgl.DGLGraph,\n",
    "        node_self_attention: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        edge_attention_projection = torch.zeros(\n",
    "            [self._num_heads, g.num_edges(), g.num_edges()])\n",
    "\n",
    "        for node in range(lg.num_edges()):\n",
    "            edges = lg.find_edges(node)\n",
    "\n",
    "            source = edges[0].item()\n",
    "            destination = edges[1].item()\n",
    "\n",
    "            connecting_node = g.find_edges(source)[1].item()\n",
    "\n",
    "            for head in range(self._num_heads):\n",
    "                attention_score = node_self_attention[head][connecting_node]\n",
    "\n",
    "                edge_attention_projection[head][source][destination] = attention_score\n",
    "\n",
    "            return edge_attention_projection\n",
    "\n",
    "    def _calculate_message_passing(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        value: torch.Tensor,\n",
    "        attention_projection: torch.Tensor,\n",
    "    ):\n",
    "        adjacency = g.adj().to_dense()\n",
    "\n",
    "        if self._message_aggregation_type == 'sum':\n",
    "            x = attention_projection * adjacency\n",
    "        elif self._message_aggregation_type == 'mean':\n",
    "            degree_inv = torch.linalg.inv(torch.diag(g.in_degrees().float()))\n",
    "\n",
    "            x = degree_inv @ attention_projection * adjacency\n",
    "        elif self._message_aggregation_type == 'gcn':\n",
    "            degree_inv_sqrt = torch.sqrt(torch.linalg.inv(\n",
    "                torch.diag(g.in_degrees().float())))\n",
    "            adjacency_inv_sqrt = torch.sqrt(torch.linalg.inv(adjacency))\n",
    "\n",
    "            x = degree_inv_sqrt @ attention_projection * \\\n",
    "                adjacency_inv_sqrt @ degree_inv_sqrt\n",
    "\n",
    "        message_passing = x @ value\n",
    "\n",
    "        return message_passing\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        lg: dgl.DGLGraph,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        node_query = self._node_query_linear(node_inputs)\n",
    "        node_query = node_query.view(self._num_heads, -1, self._node_in_feats)\n",
    "        node_key = self._node_key_linear(node_inputs)\n",
    "        node_key = node_key.view(self._num_heads, -1, 1)\n",
    "        node_value = self._node_value_linear(node_inputs)\n",
    "        node_value = node_value.view(self._num_heads, -1, self._node_in_feats)\n",
    "\n",
    "        edge_query = self._edge_query_linear(edge_inputs)\n",
    "        edge_query = edge_query.view(self._num_heads, -1, self._edge_in_feats)\n",
    "        edge_key = self._edge_key_linear(edge_inputs)\n",
    "        edge_key = edge_key.view(self._num_heads, -1, 1)\n",
    "        edge_value = self._edge_value_linear(edge_inputs)\n",
    "        edge_value = edge_value.view(self._num_heads, -1, self._edge_in_feats)\n",
    "\n",
    "        if self._short_residual:\n",
    "            node_self_attention = self._calculate_self_attention(\n",
    "                node_query, node_key, self._node_in_feats, node_inputs)\n",
    "            edge_self_attention = self._calculate_self_attention(\n",
    "                edge_query, edge_key, self._edge_in_feats, edge_inputs)\n",
    "        else:\n",
    "            node_self_attention = self._calculate_self_attention(\n",
    "                node_query, node_key, self._node_in_feats)\n",
    "            edge_self_attention = self._calculate_self_attention(\n",
    "                edge_query, edge_key, self._edge_in_feats)\n",
    "\n",
    "        node_attention_projection = self._create_node_attention_projection(\n",
    "            g, edge_self_attention)\n",
    "        edge_attention_projection = self._create_edge_attention_projection(\n",
    "            g, lg, node_self_attention)\n",
    "\n",
    "        node_message_passing = self._calculate_message_passing(\n",
    "            g, node_value, node_attention_projection)\n",
    "        edge_message_passing = self._calculate_message_passing(\n",
    "            lg, edge_value, edge_attention_projection)\n",
    "\n",
    "        node_message_passing = self._node_dropout(node_message_passing)\n",
    "        edge_message_passing = self._edge_dropout(edge_message_passing)\n",
    "\n",
    "        if self._head_pooling_type == 'sum':\n",
    "            node_message_passing = node_message_passing.sum(dim=-3)\n",
    "            edge_message_passing = edge_message_passing.sum(dim=-3)\n",
    "        elif self._head_pooling_type == 'mean':\n",
    "            node_message_passing = node_message_passing.mean(dim=-3)\n",
    "            edge_message_passing = edge_message_passing.mean(dim=-3)\n",
    "\n",
    "        return node_message_passing, edge_message_passing\n",
    "\n",
    "\n",
    "class MutualAttentionTransformerLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        node_out_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        edge_out_feats: int,\n",
    "        num_heads: int,\n",
    "        short_residual: bool,\n",
    "        long_residual: bool,\n",
    "        dropout_probability: float,\n",
    "        message_aggregation_type: str,\n",
    "        head_pooling_type: str,\n",
    "        normalization_type: str,\n",
    "        linear_projection_activation: str = None,\n",
    "        linear_embedding_activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._long_residual = long_residual\n",
    "        self._mutual_multi_attention_head = MutualMultiAttentionHead(\n",
    "            node_in_feats,\n",
    "            edge_in_feats,\n",
    "            num_heads,\n",
    "            short_residual,\n",
    "            dropout_probability,\n",
    "            message_aggregation_type,\n",
    "            head_pooling_type,\n",
    "            linear_projection_activation,\n",
    "        )\n",
    "        self._node_linear_embedding = LinearLayer(\n",
    "            node_in_feats, node_out_feats, linear_embedding_activation)\n",
    "        self._edge_linear_embedding = LinearLayer(\n",
    "            edge_in_feats, edge_out_feats, linear_embedding_activation)\n",
    "\n",
    "        if normalization_type == 'layer':\n",
    "            self._node_normalization_1 = nn.LayerNorm(node_in_feats)\n",
    "            self._node_normalization_2 = nn.LayerNorm(node_out_feats)\n",
    "\n",
    "            self._edge_normalization_1 = nn.LayerNorm(edge_in_feats)\n",
    "            self._edge_normalization_2 = nn.LayerNorm(edge_out_feats)\n",
    "        elif normalization_type == 'batch':\n",
    "            self._node_normalization_1 = nn.BatchNorm1d(node_in_feats)\n",
    "            self._node_normalization_2 = nn.BatchNorm1d(node_out_feats)\n",
    "\n",
    "            self._edge_normalization_1 = nn.BatchNorm1d(edge_in_feats)\n",
    "            self._edge_normalization_2 = nn.BatchNorm1d(edge_out_feats)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        lg: dgl.DGLGraph,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        node_embedding, edge_embedding = self._mutual_multi_attention_head(\n",
    "            g, lg, node_inputs, edge_inputs)\n",
    "\n",
    "        if self._long_residual:\n",
    "            node_embedding += node_inputs\n",
    "            edge_embedding += edge_inputs\n",
    "\n",
    "        node_embedding = self._node_normalization_1(node_embedding)\n",
    "        edge_embedding = self._edge_normalization_1(edge_embedding)\n",
    "\n",
    "        node_embedding = self._node_linear_embedding(node_embedding)\n",
    "        edge_embedding = self._edge_linear_embedding(edge_embedding)\n",
    "\n",
    "        node_embedding = self._node_normalization_2(node_embedding)\n",
    "        edge_embedding = self._edge_normalization_2(edge_embedding)\n",
    "\n",
    "        return node_embedding, edge_embedding\n",
    "\n",
    "\n",
    "class GraphMutualAttentionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        node_hidden_feats: int,\n",
    "        node_out_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        edge_hidden_feats: int,\n",
    "        edge_out_feats: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        short_residual: bool,\n",
    "        long_residual: bool,\n",
    "        dropout_probability: float,\n",
    "        message_aggregation_type: str,\n",
    "        head_pooling_type: str,\n",
    "        readout_pooling_type: str,\n",
    "        normalization_type: str,\n",
    "        linear_projection_activation: str = None,\n",
    "        linear_embedding_activation: str = None,\n",
    "        bilinear_readout_activation: str = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self._node_out_feats = node_out_feats\n",
    "        self._edge_out_feats = edge_out_feats\n",
    "        self._num_layers = num_layers\n",
    "        self._readout_pooling_type = readout_pooling_type\n",
    "        self._transformer_layers = self._create_transformer_layers(\n",
    "            node_in_feats,\n",
    "            node_hidden_feats,\n",
    "            node_out_feats,\n",
    "            edge_in_feats,\n",
    "            edge_hidden_feats,\n",
    "            edge_out_feats,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            short_residual,\n",
    "            long_residual,\n",
    "            dropout_probability,\n",
    "            message_aggregation_type,\n",
    "            head_pooling_type,\n",
    "            normalization_type,\n",
    "            linear_projection_activation,\n",
    "            linear_embedding_activation,\n",
    "        )\n",
    "        self._bilinear_readout = BilinearReadoutLayer(\n",
    "            node_out_feats, edge_out_feats, 1, bilinear_readout_activation)\n",
    "\n",
    "    def _create_transformer_layers(\n",
    "        self,\n",
    "        node_in_feats: int,\n",
    "        node_hidden_feats: int,\n",
    "        node_out_feats: int,\n",
    "        edge_in_feats: int,\n",
    "        edge_hidden_feats: int,\n",
    "        edge_out_feats: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        short_residual: bool,\n",
    "        long_residual: bool,\n",
    "        dropout_probability: float,\n",
    "        message_aggregation_type: str,\n",
    "        head_pooling_type: str,\n",
    "        normalization_type: str,\n",
    "        linear_projection_activation: str = None,\n",
    "        linear_embedding_activation: str = None,\n",
    "    ) -> nn.ModuleList:\n",
    "        transformer_layers = nn.ModuleList()\n",
    "\n",
    "        if num_layers > 1:\n",
    "            transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                node_in_feats,\n",
    "                node_hidden_feats,\n",
    "                edge_in_feats,\n",
    "                edge_hidden_feats,\n",
    "                num_heads,\n",
    "                short_residual,\n",
    "                long_residual,\n",
    "                dropout_probability,\n",
    "                message_aggregation_type,\n",
    "                head_pooling_type,\n",
    "                normalization_type,\n",
    "                linear_projection_activation,\n",
    "                linear_embedding_activation,\n",
    "            ))\n",
    "\n",
    "            for _ in range(num_layers - 2):\n",
    "                transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                    node_hidden_feats,\n",
    "                    node_hidden_feats,\n",
    "                    edge_hidden_feats,\n",
    "                    edge_hidden_feats,\n",
    "                    num_heads,\n",
    "                    short_residual,\n",
    "                    long_residual,\n",
    "                    dropout_probability,\n",
    "                    message_aggregation_type,\n",
    "                    head_pooling_type,\n",
    "                    normalization_type,\n",
    "                    linear_projection_activation,\n",
    "                    linear_embedding_activation,\n",
    "                ))\n",
    "\n",
    "            transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                node_hidden_feats,\n",
    "                node_out_feats,\n",
    "                edge_hidden_feats,\n",
    "                edge_out_feats,\n",
    "                num_heads,\n",
    "                short_residual,\n",
    "                long_residual,\n",
    "                dropout_probability,\n",
    "                message_aggregation_type,\n",
    "                head_pooling_type,\n",
    "                normalization_type,\n",
    "                linear_projection_activation,\n",
    "                linear_embedding_activation,\n",
    "            ))\n",
    "        else:\n",
    "            transformer_layers.append(MutualAttentionTransformerLayer(\n",
    "                node_in_feats,\n",
    "                node_out_feats,\n",
    "                edge_in_feats,\n",
    "                edge_out_feats,\n",
    "                num_heads,\n",
    "                short_residual,\n",
    "                long_residual,\n",
    "                dropout_probability,\n",
    "                message_aggregation_type,\n",
    "                head_pooling_type,\n",
    "                normalization_type,\n",
    "                linear_projection_activation,\n",
    "                linear_embedding_activation,\n",
    "            ))\n",
    "\n",
    "        return transformer_layers\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        g: dgl.DGLGraph,\n",
    "        lg: dgl.DGLGraph,\n",
    "        node_inputs: torch.Tensor,\n",
    "        edge_inputs: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        node_embedding = node_inputs\n",
    "        edge_embedding = edge_inputs\n",
    "\n",
    "        for transformer_layer in self._transformer_layers:\n",
    "            node_embedding, edge_embedding = transformer_layer(\n",
    "                g, lg, node_inputs, edge_inputs)\n",
    "\n",
    "        if self._readout_pooling_type == 'sum':\n",
    "            node_embedding = node_embedding.sum(dim=-2)\n",
    "            edge_embedding = edge_embedding.sum(dim=-2)\n",
    "        elif self._readout_pooling_type == 'mean':\n",
    "            node_embedding = node_embedding.mean(dim=-2)\n",
    "            edge_embedding = edge_embedding.mean(dim=-2)\n",
    "\n",
    "        readout = self._bilinear_readout(node_embedding, edge_embedding)\n",
    "\n",
    "        return readout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = default_timer()\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    g = dataset[i][0].add_self_loop()\n",
    "    lg = dgl.line_graph(g, backtracking=False).add_self_loop()\n",
    "\n",
    "    node_inputs = g.ndata['feat'].float()\n",
    "    edge_inputs = g.edata['feat'].float()\n",
    "\n",
    "stop = default_timer()\n",
    "\n",
    "print(stop - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dataset[89895][0].add_self_loop()\n",
    "lg = dgl.line_graph(g, backtracking=False).add_self_loop()\n",
    "\n",
    "node_inputs = g.ndata['feat'].float()\n",
    "edge_inputs = g.edata['feat'].float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GraphMutualAttentionTransformer(\n",
    "    node_in_feats=9,\n",
    "    node_hidden_feats=9,\n",
    "    node_out_feats=9,\n",
    "    edge_in_feats=3,\n",
    "    edge_hidden_feats=3,\n",
    "    edge_out_feats=3,\n",
    "    num_layers=5,\n",
    "    num_heads=4,\n",
    "    short_residual=True,\n",
    "    long_residual=True,\n",
    "    dropout_probability=0.01,\n",
    "    message_aggregation_type='mean',\n",
    "    head_pooling_type='sum',\n",
    "    readout_pooling_type='mean',\n",
    "    normalization_type='layer',\n",
    "    linear_projection_activation='relu',\n",
    "    linear_embedding_activation='relu',\n",
    "    bilinear_readout_activation='softplus',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.202048855517487\n"
     ]
    }
   ],
   "source": [
    "start = default_timer()\n",
    "\n",
    "for i in range(100):\n",
    "    g = dataset[i][0].add_self_loop()\n",
    "    lg = dgl.line_graph(g, backtracking=False).add_self_loop()\n",
    "\n",
    "    node_inputs = g.ndata['feat'].float()\n",
    "    edge_inputs = g.edata['feat'].float()\n",
    "\n",
    "    model(g, lg, node_inputs, edge_inputs)\n",
    "\n",
    "stop = default_timer()\n",
    "\n",
    "x = (stop - start) / 100\n",
    "print(x * len(test_idx) / 3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}